{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hopsworks Feature Store # HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section. Getting Started On Hopsworks # Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) Feed the training dataset to a TensorFlow model: tf_data_object = training_dataset . tf_data ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = tf_data_object . tf_record_dataset ( batch_size = 32 , num_epochs = 5 , process = True ) You can find more examples on how to use the library in our hops-examples repository. Documentation # Documentation is available at Hopsworks Feature Store Documentation . Issues # For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking . Contributing # If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Introduction"},{"location":"#hopsworks-feature-store","text":"HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section.","title":"Hopsworks Feature Store"},{"location":"#getting-started-on-hopsworks","text":"Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) Feed the training dataset to a TensorFlow model: tf_data_object = training_dataset . tf_data ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = tf_data_object . tf_record_dataset ( batch_size = 32 , num_epochs = 5 , process = True ) You can find more examples on how to use the library in our hops-examples repository.","title":"Getting Started On Hopsworks"},{"location":"#documentation","text":"Documentation is available at Hopsworks Feature Store Documentation .","title":"Documentation"},{"location":"#issues","text":"For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking .","title":"Issues"},{"location":"#contributing","text":"If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Contributing"},{"location":"CONTRIBUTING/","text":"Python development setup # Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 : cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hopsworks black hopsworks Python documentation # We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults. Setup and Build Documentation # We use mkdocs to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Either build the docs, or serve them dynamically: mkdocs build # or mkdocs serve Adding new API documentation # To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Contributing"},{"location":"CONTRIBUTING/#python-development-setup","text":"Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 : cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hopsworks black hopsworks","title":"Python development setup"},{"location":"CONTRIBUTING/#python-documentation","text":"We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults.","title":"Python documentation"},{"location":"CONTRIBUTING/#setup-and-build-documentation","text":"We use mkdocs to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Either build the docs, or serve them dynamically: mkdocs build # or mkdocs serve","title":"Setup and Build Documentation"},{"location":"CONTRIBUTING/#adding-new-api-documentation","text":"To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Adding new API documentation"},{"location":"overview/","text":"Concept Overview # Project-Based Multi-tenancy # Hopsworks implements a dynamic role-based access control model through a project-based multi-tenant security model . Inspired by GDPR, in Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Every Project has an owner with full read-write privileges and zero or more members. %% change to github link An important aspect of Project based multi-tenancy is that assets can be shared between projects. The current assets that can be shared between projects are: files/directories in HopsFS, Hive databases, feature stores , and Kafka topics. Important Sharing assets does not mean that data is duplicated. The Hopsworks Feature Store # The Hopsworks Feature Store is a tool for curating and serving machine learning (ML)features. The Feature Store is a central and unified API between Data Engineers and Data Scientists. Benefits of the Feature Store Manage feature data to profive unified access to machine learning features from small teams to large enterprises. Enable discovery, documentation, sharing and insights into your features through rich metadata. Make feature data available in a performant and scalable way for model training and model inference. Allow point-in-time correct and consistent access to feature data (time travel). Feature Store Concepts # Entities in the Feature Store Entities within the Feature Store are organized hierarchically. On the most granular level are the features itself. Data Engineers ingest the feature data within their organization through the creation of feature groups . Data Scientists are then able to read selected features from the feature groups to create training datasets for model training, run batch inference with deployed models or perform inference from online models by scoring single feature vectors . Feature Vector A Feature Vector is a single row of feature values associated with a primary key. Feature Groups # Feature Groups are entities that contain both metadata about the grouped features, as well as information of the jobs used to ingest the data contained in a feature group and also the actual location of the data (HopsFS or externally, such as S3). Typically, feature groups represent a logical set of features coming from the same data source sharing a common primary key. Feature groups also contain the schema and type information of the features, for the user to know how to interpret the data. Feature groups can also be used to compute Statistics over features, or to define Data Validation Rules using the statistics and schema information. In order to enable online serving for features of a feature group, the feature group needs to be made available as an online feature group. Training Datasets # In order to be able to train machine learning models efficiently, the feature data needs to be materialized as a Training Dataset in the file format most suitable for the ML framework used. For example, when training models with TensorFlow the ideal file format is TensorFlow's tfrecord format. Training datasets can be created with features from any number of feature groups, as long as the feature groups can be joined in a meaningful way. Users are able to compute Statistics also for training datasets, which will make it easy to understand a dataset's characteristics also in the future. The Hopsworks Feature Store has support for writing training datasets either to the distributed file system of Hopsworks - HopsFS - or to external storage such as S3. Offline vs. Online Feature Store # The Feature Store is a dual database-system, to cover all machine learning use cases it consists of high throughput offline storage layer, and additionally a low-latency online storage. The offline storage is mainly used to generate large batches of feature data, for example to be exported as training datasets. Additionally, the offline storage can be used to score large amounts of data with a machine learning model in regular intervals, so called batch inference . The online storage on the other hand is required for online applications, where the goal is to retrieve a single feature vector with the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. An example for online inference would be an e-commerce business, which would like to predict the credit score of a client when he is about to checkout his shopping cart. A client-id will be sent to the online feature store to retrieve the historic features for this customer, which can then be enriched by real time features like the value of his shopping cart, and will then be passed to the machine learning model for inference. Offline vs. Online Feature Store There is no database fullfilling both requirements of very low latency and and high throughput. Therefore, the Hopsworks Feature Store builds on Apache Hive with Apache Hudi as offline storage layer and MySQL Cluster (NDB) as online storage.","title":"Overview"},{"location":"overview/#concept-overview","text":"","title":"Concept Overview"},{"location":"overview/#project-based-multi-tenancy","text":"Hopsworks implements a dynamic role-based access control model through a project-based multi-tenant security model . Inspired by GDPR, in Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Every Project has an owner with full read-write privileges and zero or more members. %% change to github link An important aspect of Project based multi-tenancy is that assets can be shared between projects. The current assets that can be shared between projects are: files/directories in HopsFS, Hive databases, feature stores , and Kafka topics. Important Sharing assets does not mean that data is duplicated.","title":"Project-Based Multi-tenancy"},{"location":"overview/#the-hopsworks-feature-store","text":"The Hopsworks Feature Store is a tool for curating and serving machine learning (ML)features. The Feature Store is a central and unified API between Data Engineers and Data Scientists. Benefits of the Feature Store Manage feature data to profive unified access to machine learning features from small teams to large enterprises. Enable discovery, documentation, sharing and insights into your features through rich metadata. Make feature data available in a performant and scalable way for model training and model inference. Allow point-in-time correct and consistent access to feature data (time travel).","title":"The Hopsworks Feature Store"},{"location":"overview/#feature-store-concepts","text":"Entities in the Feature Store Entities within the Feature Store are organized hierarchically. On the most granular level are the features itself. Data Engineers ingest the feature data within their organization through the creation of feature groups . Data Scientists are then able to read selected features from the feature groups to create training datasets for model training, run batch inference with deployed models or perform inference from online models by scoring single feature vectors . Feature Vector A Feature Vector is a single row of feature values associated with a primary key.","title":"Feature Store Concepts"},{"location":"overview/#feature-groups","text":"Feature Groups are entities that contain both metadata about the grouped features, as well as information of the jobs used to ingest the data contained in a feature group and also the actual location of the data (HopsFS or externally, such as S3). Typically, feature groups represent a logical set of features coming from the same data source sharing a common primary key. Feature groups also contain the schema and type information of the features, for the user to know how to interpret the data. Feature groups can also be used to compute Statistics over features, or to define Data Validation Rules using the statistics and schema information. In order to enable online serving for features of a feature group, the feature group needs to be made available as an online feature group.","title":"Feature Groups"},{"location":"overview/#training-datasets","text":"In order to be able to train machine learning models efficiently, the feature data needs to be materialized as a Training Dataset in the file format most suitable for the ML framework used. For example, when training models with TensorFlow the ideal file format is TensorFlow's tfrecord format. Training datasets can be created with features from any number of feature groups, as long as the feature groups can be joined in a meaningful way. Users are able to compute Statistics also for training datasets, which will make it easy to understand a dataset's characteristics also in the future. The Hopsworks Feature Store has support for writing training datasets either to the distributed file system of Hopsworks - HopsFS - or to external storage such as S3.","title":"Training Datasets"},{"location":"overview/#offline-vs-online-feature-store","text":"The Feature Store is a dual database-system, to cover all machine learning use cases it consists of high throughput offline storage layer, and additionally a low-latency online storage. The offline storage is mainly used to generate large batches of feature data, for example to be exported as training datasets. Additionally, the offline storage can be used to score large amounts of data with a machine learning model in regular intervals, so called batch inference . The online storage on the other hand is required for online applications, where the goal is to retrieve a single feature vector with the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. An example for online inference would be an e-commerce business, which would like to predict the credit score of a client when he is about to checkout his shopping cart. A client-id will be sent to the online feature store to retrieve the historic features for this customer, which can then be enriched by real time features like the value of his shopping cart, and will then be passed to the machine learning model for inference. Offline vs. Online Feature Store There is no database fullfilling both requirements of very low latency and and high throughput. Therefore, the Hopsworks Feature Store builds on Apache Hive with Apache Hudi as offline storage layer and MySQL Cluster (NDB) as online storage.","title":"Offline vs. Online Feature Store"},{"location":"quickstart/","text":"Quickstart Guide # The Hopsworks feature store is a centralized repository, within an organization, to manage machine learning features. A feature is a measurable property of a phenomenon. It could be a simple value such as the age of a customer, or it could be an aggregated value, such as the number of transactions made by a customer in the last 30 days. A feature is not restricted to an numeric value, it could be a string representing an address, or an image. The Hopsworks Feature Store A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models. In this Quickstart Guide we are going to focus on the left side of the picture above. In particular how data engeneers can create features and push them to the Hopsworks feature store so that they are available to the data scientists HSFS library # The Hopsworks feature feature store library is called hsfs ( H opswork s F eature S tore). The library is Apache V2 licensed and available here . The library is currently available for Python and JVM languages such as Scala and Java. If you want to connect to the Feature Store from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Feature Store. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Feature Store. In fact, the Feature Store itself is also represented by an object. Furthermore, these objects have methods to save data along with the entities in the feature store. This data can be materialized from Spark or Pandas DataFrames, or the HSFS - Query abstraction . Guide Notebooks # This guide is based on a series of notebooks , which is available in the Feature Store Demo Tour Project on Hopsworks. Connection, Project and Feature Store # The first step is to establish a connection with your Hopsworks Feature Store instance and retrieve the object that represents the Feature Store you'll be working with. By default connection.get_feature_store() returns the feature store of the project you are working with. However, it accepts also a project name as parameter to select a different feature store. Python import hsfs # Create a connection connection = hsfs . connection () # Get the feature store handle for the project's feature store fs = connection . get_feature_store () Scala import com.logicalclocks.hsfs._ import scala.collection.JavaConverters._ // Create a connection val connection = HopsworksConnection . builder (). build (); // Get the feature store handle for the project's feature store val fs = connection . getFeatureStore (); You can inspect the Feature Store's meta data by accessing its attributes: Python print ( fs . name ) print ( fs . description ) Scala println ( fs . getName ) println ( fs . getDescription ) Example Data # In order to use the example data, you need to unzip the archive.zip file which is located in /Jupyter/hsfs/ when you are running the Quickstart from the Feature Store Demo Tour project. To do so, head to the Data Sets tab in Hopsworks, open the /Jupyter/hsfs directory, mark the archive.zip -file and click the extract button. The Data Sets browser Of course you can also use your own data if you read it into a Spark DataFrame. Feature Groups # Assuming you have done some feature engineering on the raw data, having produced a DataFrame with Features, these can now be saved to the Feature Store. For exmaples of feature engineering on the provided Sales data, see the example notebook . Creation # Create a feature group named store_fg . The store is the primary key uniquely identifying all the remaining features in this feature group. As you can see, you have the possibility to make settings on the Feature Group, such as the version number, or the statistics which should be computed. The Feature Group Guide guides through the full configuration of Feature Groups. Python store_fg_meta = fs . create_feature_group ( name = \"store_fg\" , version = 1 , primary_key = [ \"store\" ], description = \"Store related features\" , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True }) Scala val storeFgMeta = ( fs . createFeatureGroup () . name ( \"store_fg\" ) . description ( \"Store related features\" ) . version ( 1 ) . primaryKeys ( Seq ( \"store\" ). asJava ) . statisticsEnabled ( True ) . histograms ( True ) . correlations ( True ) . build ()) Up to this point we have just created the metadata object representing the feature group. However, we haven't saved the feature group in the feature store yet. To do so, we can call the method save on the metadata object created in the cell above. Python store_fg_meta . save ( store_dataframe ) Scala storeFgMeta . save ( store_dataframe ) Retrieval # If there were feature groups previously created in your Feature Store, or you want to pick up where you left off before, you can retrieve and read feature groups in a similar fashion as creating them: Using the Feature Store object, you can retrieve handles to the entities, such as feature groups, in the Feature Store. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. This is necessary, in order to make the code reproducible, as version changes indicate breaking schema changes. Python exogenous_fg_meta = fs . get_feature_group ( 'exogenous_fg' , version = 1 ) # Read the data, by default selecting all features exogenous_df = exogenous_fg_meta . read () # Select a subset of features and read into dataframe exogenous_df_subset = exogenous_fg_meta . select ([ \"store\" , \"fuel_price\" , \"is_holiday\" ]) . read () Scala val exogenousFgMeta = fs . getFeatureGroup ( \"exogenous_fg\" , 1 ) // Read the data, by default selecting all features val exogenousDf = exogenousFgMeta . read () // Select a subset of features and read into dataframe val exogenousDfSubset = exogenousFgMeta . select ( Seq ( \"store\" , \"fuel_price\" , \"is_holiday\" ). asJava ). read () Joining # HSFS provides an API similar to Pandas to join feature groups together and to select features from different feature groups. The easies query you can write is by selecting all the features from a feature group and join them with all the features of another feature group. You can use the select_all() method of a feature group to select all its features. HSFS relies on the Hopsworks feature store to identify which features of the two feature groups to use as joining condition. If you don't specify anything, Hopsworks will use the largest matching subset of primary keys with the same name. In the example below, sales_fg has store , dept and date as composite primary key while exogenous_fg has only store and date . So Hopsworks will set as joining condition store and date . Python sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () . join ( exogenous_fg . select_all ()) # print first 5 rows of the query query . show ( 5 ) Scala val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) val query = salesFg . selectAll (). join ( exogenousFg . selectAll ()) // print first 5 rows of the query query . show ( 5 ) For a more complex joins, and details about overwriting the join keys and join type, the programming interface guide explains the Query interface as well as Training Datasets # Once a Data Scientist has found the features she needs for her model, she can create a training dataset to materialize the features in the desired file format. The Hopsworks Feature Store supports a variety of file formats, matching the Data Scientists' favourite Machine Learning Frameworks. Creation # You can either create a training dataset from a Query object or directly from a Spark or Pandas DataFrame. Spark and Pandas give you more flexibility, but it has drawbacks for reproducability at inference time, when the Feature Vector needs to be reconstructed. The idea of the Feature Store is to have ready-engineered features available for Data Scientists to be selected for training datasets. With this assumption, it should not be necessary to perform additional engineering, but instead joining, filtering and point in time querying should be enough to generate training datasets. Python store_fg = fs . get_feature_group ( \"store_fg\" ) sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () \\ . join ( store_fg . select_all ()) \\ . join ( exogenous_fg . select ([ 'fuel_price' , 'unemployment' , 'cpi' ])) td = fs . create_training_dataset ( name = \"sales_model\" , description = \"Dataset to train the sales model\" , data_format = \"tfrecord\" , splits = { \"train\" : 0.7 , \"test\" : 0.2 , \"validate\" : 0.1 }, version = 1 ) td . save ( query ) Scala val storeFg = fs . getFeatureGroup ( \"store_fg\" ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) query = ( salesFg . selectAll () . join ( storeFg . selectAll ()) . join ( exogenousFg . select ( Seq ( \"fuel_price\" , \"unemployment\" , \"cpi\" ). asJava ))) val td = ( fs . createTrainingDataset () . name ( \"sales_model\" ) . description ( \"Dataset to train the sales model\" ) . version ( 1 ) . dataFormat ( DataFormat . TFRECORD ) . splits ( Map ( \"train\" -> Double . box ( 0.7 ), \"test\" -> Double . box ( 0.2 ), \"validate\" -> Double . box ( 0.1 )) . build ()) td . save ( query ) Retrieval # If you want to use a previously created training dataset to train a machine learning model, you can get the training dataset similarly to how you get a feature group. Python td = fs . get_training_dataset ( \"sales_model\" ) df = td . read ( split = \"train\" ) Scala val td = fs . getTrainingDataset ( \"sales_model\" ) val df = td . read ( \"train\" ) Either you read the data into a DataFrame again, or you use the provided utility methods, to instantiate for example a tf.data.Dataset , which can directly be passed to a TensorFlow model. Python train_input_feeder = training_dataset . feed ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = train_input_feeder . tf_record_dataset () Scala This functionality is only available in the Python API.","title":"Quickstart"},{"location":"quickstart/#quickstart-guide","text":"The Hopsworks feature store is a centralized repository, within an organization, to manage machine learning features. A feature is a measurable property of a phenomenon. It could be a simple value such as the age of a customer, or it could be an aggregated value, such as the number of transactions made by a customer in the last 30 days. A feature is not restricted to an numeric value, it could be a string representing an address, or an image. The Hopsworks Feature Store A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models. In this Quickstart Guide we are going to focus on the left side of the picture above. In particular how data engeneers can create features and push them to the Hopsworks feature store so that they are available to the data scientists","title":"Quickstart Guide"},{"location":"quickstart/#hsfs-library","text":"The Hopsworks feature feature store library is called hsfs ( H opswork s F eature S tore). The library is Apache V2 licensed and available here . The library is currently available for Python and JVM languages such as Scala and Java. If you want to connect to the Feature Store from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Feature Store. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Feature Store. In fact, the Feature Store itself is also represented by an object. Furthermore, these objects have methods to save data along with the entities in the feature store. This data can be materialized from Spark or Pandas DataFrames, or the HSFS - Query abstraction .","title":"HSFS library"},{"location":"quickstart/#guide-notebooks","text":"This guide is based on a series of notebooks , which is available in the Feature Store Demo Tour Project on Hopsworks.","title":"Guide Notebooks"},{"location":"quickstart/#connection-project-and-feature-store","text":"The first step is to establish a connection with your Hopsworks Feature Store instance and retrieve the object that represents the Feature Store you'll be working with. By default connection.get_feature_store() returns the feature store of the project you are working with. However, it accepts also a project name as parameter to select a different feature store. Python import hsfs # Create a connection connection = hsfs . connection () # Get the feature store handle for the project's feature store fs = connection . get_feature_store () Scala import com.logicalclocks.hsfs._ import scala.collection.JavaConverters._ // Create a connection val connection = HopsworksConnection . builder (). build (); // Get the feature store handle for the project's feature store val fs = connection . getFeatureStore (); You can inspect the Feature Store's meta data by accessing its attributes: Python print ( fs . name ) print ( fs . description ) Scala println ( fs . getName ) println ( fs . getDescription )","title":"Connection, Project and Feature Store"},{"location":"quickstart/#example-data","text":"In order to use the example data, you need to unzip the archive.zip file which is located in /Jupyter/hsfs/ when you are running the Quickstart from the Feature Store Demo Tour project. To do so, head to the Data Sets tab in Hopsworks, open the /Jupyter/hsfs directory, mark the archive.zip -file and click the extract button. The Data Sets browser Of course you can also use your own data if you read it into a Spark DataFrame.","title":"Example Data"},{"location":"quickstart/#feature-groups","text":"Assuming you have done some feature engineering on the raw data, having produced a DataFrame with Features, these can now be saved to the Feature Store. For exmaples of feature engineering on the provided Sales data, see the example notebook .","title":"Feature Groups"},{"location":"quickstart/#creation","text":"Create a feature group named store_fg . The store is the primary key uniquely identifying all the remaining features in this feature group. As you can see, you have the possibility to make settings on the Feature Group, such as the version number, or the statistics which should be computed. The Feature Group Guide guides through the full configuration of Feature Groups. Python store_fg_meta = fs . create_feature_group ( name = \"store_fg\" , version = 1 , primary_key = [ \"store\" ], description = \"Store related features\" , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True }) Scala val storeFgMeta = ( fs . createFeatureGroup () . name ( \"store_fg\" ) . description ( \"Store related features\" ) . version ( 1 ) . primaryKeys ( Seq ( \"store\" ). asJava ) . statisticsEnabled ( True ) . histograms ( True ) . correlations ( True ) . build ()) Up to this point we have just created the metadata object representing the feature group. However, we haven't saved the feature group in the feature store yet. To do so, we can call the method save on the metadata object created in the cell above. Python store_fg_meta . save ( store_dataframe ) Scala storeFgMeta . save ( store_dataframe )","title":"Creation"},{"location":"quickstart/#retrieval","text":"If there were feature groups previously created in your Feature Store, or you want to pick up where you left off before, you can retrieve and read feature groups in a similar fashion as creating them: Using the Feature Store object, you can retrieve handles to the entities, such as feature groups, in the Feature Store. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. This is necessary, in order to make the code reproducible, as version changes indicate breaking schema changes. Python exogenous_fg_meta = fs . get_feature_group ( 'exogenous_fg' , version = 1 ) # Read the data, by default selecting all features exogenous_df = exogenous_fg_meta . read () # Select a subset of features and read into dataframe exogenous_df_subset = exogenous_fg_meta . select ([ \"store\" , \"fuel_price\" , \"is_holiday\" ]) . read () Scala val exogenousFgMeta = fs . getFeatureGroup ( \"exogenous_fg\" , 1 ) // Read the data, by default selecting all features val exogenousDf = exogenousFgMeta . read () // Select a subset of features and read into dataframe val exogenousDfSubset = exogenousFgMeta . select ( Seq ( \"store\" , \"fuel_price\" , \"is_holiday\" ). asJava ). read ()","title":"Retrieval"},{"location":"quickstart/#joining","text":"HSFS provides an API similar to Pandas to join feature groups together and to select features from different feature groups. The easies query you can write is by selecting all the features from a feature group and join them with all the features of another feature group. You can use the select_all() method of a feature group to select all its features. HSFS relies on the Hopsworks feature store to identify which features of the two feature groups to use as joining condition. If you don't specify anything, Hopsworks will use the largest matching subset of primary keys with the same name. In the example below, sales_fg has store , dept and date as composite primary key while exogenous_fg has only store and date . So Hopsworks will set as joining condition store and date . Python sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () . join ( exogenous_fg . select_all ()) # print first 5 rows of the query query . show ( 5 ) Scala val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) val query = salesFg . selectAll (). join ( exogenousFg . selectAll ()) // print first 5 rows of the query query . show ( 5 ) For a more complex joins, and details about overwriting the join keys and join type, the programming interface guide explains the Query interface as well as","title":"Joining"},{"location":"quickstart/#training-datasets","text":"Once a Data Scientist has found the features she needs for her model, she can create a training dataset to materialize the features in the desired file format. The Hopsworks Feature Store supports a variety of file formats, matching the Data Scientists' favourite Machine Learning Frameworks.","title":"Training Datasets"},{"location":"quickstart/#creation_1","text":"You can either create a training dataset from a Query object or directly from a Spark or Pandas DataFrame. Spark and Pandas give you more flexibility, but it has drawbacks for reproducability at inference time, when the Feature Vector needs to be reconstructed. The idea of the Feature Store is to have ready-engineered features available for Data Scientists to be selected for training datasets. With this assumption, it should not be necessary to perform additional engineering, but instead joining, filtering and point in time querying should be enough to generate training datasets. Python store_fg = fs . get_feature_group ( \"store_fg\" ) sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () \\ . join ( store_fg . select_all ()) \\ . join ( exogenous_fg . select ([ 'fuel_price' , 'unemployment' , 'cpi' ])) td = fs . create_training_dataset ( name = \"sales_model\" , description = \"Dataset to train the sales model\" , data_format = \"tfrecord\" , splits = { \"train\" : 0.7 , \"test\" : 0.2 , \"validate\" : 0.1 }, version = 1 ) td . save ( query ) Scala val storeFg = fs . getFeatureGroup ( \"store_fg\" ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) query = ( salesFg . selectAll () . join ( storeFg . selectAll ()) . join ( exogenousFg . select ( Seq ( \"fuel_price\" , \"unemployment\" , \"cpi\" ). asJava ))) val td = ( fs . createTrainingDataset () . name ( \"sales_model\" ) . description ( \"Dataset to train the sales model\" ) . version ( 1 ) . dataFormat ( DataFormat . TFRECORD ) . splits ( Map ( \"train\" -> Double . box ( 0.7 ), \"test\" -> Double . box ( 0.2 ), \"validate\" -> Double . box ( 0.1 )) . build ()) td . save ( query )","title":"Creation"},{"location":"quickstart/#retrieval_1","text":"If you want to use a previously created training dataset to train a machine learning model, you can get the training dataset similarly to how you get a feature group. Python td = fs . get_training_dataset ( \"sales_model\" ) df = td . read ( split = \"train\" ) Scala val td = fs . getTrainingDataset ( \"sales_model\" ) val df = td . read ( \"train\" ) Either you read the data into a DataFrame again, or you use the provided utility methods, to instantiate for example a tf.data.Dataset , which can directly be passed to a TensorFlow model. Python train_input_feeder = training_dataset . feed ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = train_input_feeder . tf_record_dataset () Scala This functionality is only available in the Python API.","title":"Retrieval"},{"location":"setup/","text":"Integrations # Hopsworks # If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide . Databricks # Connecting to the Feature Store from Databricks requires setting up a Feature Store API Key for Databricks and installing one of the HSFS client libraries on your Databricks cluster. The Databricks integration guide explains step by step how to connect to the Feature Store from Databricks. AWS Sagemaker # Connecting to the Feature Store from SageMaker requires setting up a Feature Store API Key for SageMaker and installing the HSFS Python client library on SageMaker. The AWS SageMaker integration guide explains step by step how to connect to the Feature Store from SageMaker. Python (Local or KubeFlow) # Connecting to the Feature Store from any Python environment, such as your local environment or KubeFlow, requires setting up a Feature Store API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment. Spark Cluster # Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Overview"},{"location":"setup/#integrations","text":"","title":"Integrations"},{"location":"setup/#hopsworks","text":"If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide .","title":"Hopsworks"},{"location":"setup/#databricks","text":"Connecting to the Feature Store from Databricks requires setting up a Feature Store API Key for Databricks and installing one of the HSFS client libraries on your Databricks cluster. The Databricks integration guide explains step by step how to connect to the Feature Store from Databricks.","title":"Databricks"},{"location":"setup/#aws-sagemaker","text":"Connecting to the Feature Store from SageMaker requires setting up a Feature Store API Key for SageMaker and installing the HSFS Python client library on SageMaker. The AWS SageMaker integration guide explains step by step how to connect to the Feature Store from SageMaker.","title":"AWS Sagemaker"},{"location":"setup/#python-local-or-kubeflow","text":"Connecting to the Feature Store from any Python environment, such as your local environment or KubeFlow, requires setting up a Feature Store API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment.","title":"Python (Local or KubeFlow)"},{"location":"setup/#spark-cluster","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Spark Cluster"},{"location":"generated/feature/","text":"Feature # Features are the most granular entity in the feature store and are logically grouped by feature groups . The storage location of a single feature is determined by the feature group . Hence, enabling a feature group for online storage will make a feature available as an online feature. New features can be appended to feature groups , however, to drop features, a new feature group version has to be created. When appending features it is possible to specify a default value which is used for existing feature vectors in the feature group for the new feature. [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Feature Types # Each features requires at least an offline type to be specified for the creation of the meta data of the feature group in the offline storage, even if the feature group is going to be a purely online feature group with no data in the offline storage. Offline Storage # The offline storage is based on Apache Hive and hence, any Hive Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential offline types are: \"None\" , \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\" Online Storage # The online storage is based on MySQL Cluster (NDB) and hence, any MySQL Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(25)\" , \"VARCHAR(125)\" , \"VARCHAR(225)\" , \"VARCHAR(500)\" , \"VARCHAR(1000)\" , \"VARCHAR(2000)\" , \"VARCHAR(5000)\" , \"VARCHAR(10000)\" , \"BINARY\" , \"VARBINARY(100)\" , \"VARBINARY(500)\" , \"VARBINARY(1000)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\" Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] json # Feature . json ()","title":"Feature"},{"location":"generated/feature/#feature","text":"Features are the most granular entity in the feature store and are logically grouped by feature groups . The storage location of a single feature is determined by the feature group . Hence, enabling a feature group for online storage will make a feature available as an online feature. New features can be appended to feature groups , however, to drop features, a new feature group version has to be created. When appending features it is possible to specify a default value which is used for existing feature vectors in the feature group for the new feature. [source]","title":"Feature"},{"location":"generated/feature/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/feature/#feature-types","text":"Each features requires at least an offline type to be specified for the creation of the meta data of the feature group in the offline storage, even if the feature group is going to be a purely online feature group with no data in the offline storage.","title":"Feature Types"},{"location":"generated/feature/#offline-storage","text":"The offline storage is based on Apache Hive and hence, any Hive Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential offline types are: \"None\" , \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\"","title":"Offline Storage"},{"location":"generated/feature/#online-storage","text":"The online storage is based on MySQL Cluster (NDB) and hence, any MySQL Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(25)\" , \"VARCHAR(125)\" , \"VARCHAR(225)\" , \"VARCHAR(500)\" , \"VARCHAR(1000)\" , \"VARCHAR(2000)\" , \"VARCHAR(5000)\" , \"VARCHAR(10000)\" , \"BINARY\" , \"VARBINARY(100)\" , \"VARBINARY(500)\" , \"VARBINARY(1000)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\"","title":"Online Storage"},{"location":"generated/feature/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/feature/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/feature/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/feature/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/feature/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/feature/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/feature/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/feature/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature/#json","text":"Feature . json ()","title":"json"},{"location":"generated/feature_group/","text":"Feature Group # A Feature Groups is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The Feature Group lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them. Generally, the features in a feature froup are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, feature groups provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in. Combine features from any number of feature groups Feature groups are logical groupings of features, usually based on the data source and ingestion job from which they originate. It is important to note that feature groups are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets. Versioning # Feature groups can be versioned. Data Engineers should use the version to indicate to a Data Scientist that the schema or the feature engineering logic of the features in this group has changed. Breaking feature group schema changes In order to guarantee reproducability, the schema of a feature group should be immutable, because deleting features could lead to failing model pipelines downstream. Hence, in order to modify a schema, a new version of a feature group has to be created. In contrary, appending features to feature groups is considered a non-breaking change, since the feature store makes all selections explicit and because the namespace within a feature group is flat, it is not possible to append a new feature with an already existing name to a feature group. Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] description # Description of the feature group contents. [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Schema information. [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] name # Name of the feature group. [source] online_enabled # Setting if the feature group is available in online storage. [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] time_travel_format # Setting of the feature group time travel format. [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value = None ) Attach a name/value tag to a feature group. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . Raises RestAPIError . [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source] commit_details # FeatureGroup . commit_details ( limit = None ) Retrieves commit timeline for this feature group. Arguments limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . [source] compute_statistics # FeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag from a feature group. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. Raises RestAPIError . [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name = None ) Get the tags of a feature group. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name Optional[str] : Name of the tag to get, defaults to None . Returns list[Tag] . List of tags as name/value pairs. Raises RestAPIError . [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Updated feature group metadata object. [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature group's with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. [source] save # FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Returns the persisted FeatureGroup metadata object. Raises RestAPIError . Unable to create feature group. [source] select # FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError .","title":"Feature Group"},{"location":"generated/feature_group/#feature-group","text":"A Feature Groups is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The Feature Group lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them. Generally, the features in a feature froup are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, feature groups provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in. Combine features from any number of feature groups Feature groups are logical groupings of features, usually based on the data source and ingestion job from which they originate. It is important to note that feature groups are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets.","title":"Feature Group"},{"location":"generated/feature_group/#versioning","text":"Feature groups can be versioned. Data Engineers should use the version to indicate to a Data Scientist that the schema or the feature engineering logic of the features in this group has changed. Breaking feature group schema changes In order to guarantee reproducability, the schema of a feature group should be immutable, because deleting features could lead to failing model pipelines downstream. Hence, in order to modify a schema, a new version of a feature group has to be created. In contrary, appending features to feature groups is considered a non-breaking change, since the feature store makes all selections explicit and because the namespace within a feature group is flat, it is not possible to append a new feature with an already existing name to a feature group.","title":"Versioning"},{"location":"generated/feature_group/#creation","text":"[source]","title":"Creation"},{"location":"generated/feature_group/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object.","title":"create_feature_group"},{"location":"generated/feature_group/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_group/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/feature_group/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_group/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/feature_group/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/feature_group/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/feature_group/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/feature_group/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/feature_group/#features","text":"Schema information. [source]","title":"features"},{"location":"generated/feature_group/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/feature_group/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/feature_group/#location","text":"[source]","title":"location"},{"location":"generated/feature_group/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/feature_group/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/feature_group/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/feature_group/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/feature_group/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/feature_group/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/feature_group/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/feature_group/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/feature_group/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_group/#add_tag","text":"FeatureGroup . add_tag ( name , value = None ) Attach a name/value tag to a feature group. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . Raises RestAPIError . [source]","title":"add_tag"},{"location":"generated/feature_group/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source]","title":"append_features"},{"location":"generated/feature_group/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/feature_group/#commit_details","text":"FeatureGroup . commit_details ( limit = None ) Retrieves commit timeline for this feature group. Arguments limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . [source]","title":"commit_details"},{"location":"generated/feature_group/#compute_statistics","text":"FeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/feature_group/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/feature_group/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag from a feature group. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. Raises RestAPIError . [source]","title":"delete_tag"},{"location":"generated/feature_group/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/feature_group/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/feature_group/#get_statistics","text":"FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/feature_group/#get_tag","text":"FeatureGroup . get_tag ( name = None ) Get the tags of a feature group. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name Optional[str] : Name of the tag to get, defaults to None . Returns list[Tag] . List of tags as name/value pairs. Raises RestAPIError . [source]","title":"get_tag"},{"location":"generated/feature_group/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Updated feature group metadata object. [source]","title":"insert"},{"location":"generated/feature_group/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/feature_group/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature group's with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. [source]","title":"read_changes"},{"location":"generated/feature_group/#save","text":"FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Returns the persisted FeatureGroup metadata object. Raises RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/feature_group/#select","text":"FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/feature_group/#select_all","text":"FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/feature_group/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/feature_group/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/feature_group/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/feature_group/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/feature_store/","text":"Feature Store # Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] description # Description of the feature store. [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False )","title":"Feature Store"},{"location":"generated/feature_store/#feature-store","text":"","title":"Feature Store"},{"location":"generated/feature_store/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_store/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/feature_store/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_store/#description","text":"Description of the feature store. [source]","title":"description"},{"location":"generated/feature_store/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/feature_store/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/feature_store/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/feature_store/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/feature_store/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/feature_store/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/feature_store/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/feature_store/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/feature_store/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/feature_store/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_store/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/feature_store/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/feature_store/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/feature_store/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/feature_store/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/feature_store/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/feature_store/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/feature_store/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/feature_store/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False )","title":"sql"},{"location":"generated/project/","text":"Project/Connection # In Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Feature Store. However, it is possible to share Feature Stores among projects. When working with the Feature Store from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Feature Stores simultaneously. A connection to a Hopsworks instance is represented by a Connection object . Its main purpose is to retrieve the API Key if you are connecting from an external environment and subsequently to retrieve the needed certificates to communicate with the Feature Store services. The handle can then be used to retrieve a reference to the Feature Store you want to operate on. Examples # Python Connecting from Hopsworks import hsfs conn = hsfs . connection () fs = conn . get_feature_store () Connecting from Databricks In order to connect from Databricks, follow the integration guide . You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: Azure Use this method when working with Hopsworks on Azure. import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from AWS SageMaker In order to connect from SageMaker, follow the integration guide to setup the API Key. You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from Python environment To connect from a simple Python environment, you can provide the API Key as a file as shown in the SageMaker example above, or you provide the value directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_value = ( \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" \"xNnAzJ7RV6H\" ) ) fs = conn . get_feature_store () Scala Connecting from Hopsworks import com.logicalclocks.hsfs._ val connection = HopsworksConnection . builder (). build (); val fs = connection . getFeatureStore (); Connecting from Databricks TBD Connecting from AWS SageMaker The Scala client version of hsfs is not supported on AWS SageMaker, please use the Python client. Sharing a Feature Store # Connections are on a project-level, however, it is possible to share feature stores among projects, so even if you have a connection to one project, you can retireve a handle to any feature store shared with that project. To share a feature store, you can follow these steps: Sharing a Feature Store Open the project of the feature store that you would like to share on Hopsworks. Go to the Data Set browser and right click the Featurestore.db entry. Click Share with , then select Project and choose the project you wish to share the feature store with. Select the permissions level that the project user members should have on the feature store and click Share . Open the project you just shared the feature store with. Go to the Data Sets browser and there you should see the shared feature store as [project_name_of_shared_feature_store]::Featurestore.db . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this feature store from the other project. Sharing a feature store between projects Accepting a shared feature store from a project Connection Handle # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source] setup_databricks # Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"Project/Connection"},{"location":"generated/project/#projectconnection","text":"In Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Feature Store. However, it is possible to share Feature Stores among projects. When working with the Feature Store from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Feature Stores simultaneously. A connection to a Hopsworks instance is represented by a Connection object . Its main purpose is to retrieve the API Key if you are connecting from an external environment and subsequently to retrieve the needed certificates to communicate with the Feature Store services. The handle can then be used to retrieve a reference to the Feature Store you want to operate on.","title":"Project/Connection"},{"location":"generated/project/#examples","text":"Python Connecting from Hopsworks import hsfs conn = hsfs . connection () fs = conn . get_feature_store () Connecting from Databricks In order to connect from Databricks, follow the integration guide . You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: Azure Use this method when working with Hopsworks on Azure. import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from AWS SageMaker In order to connect from SageMaker, follow the integration guide to setup the API Key. You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from Python environment To connect from a simple Python environment, you can provide the API Key as a file as shown in the SageMaker example above, or you provide the value directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_value = ( \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" \"xNnAzJ7RV6H\" ) ) fs = conn . get_feature_store () Scala Connecting from Hopsworks import com.logicalclocks.hsfs._ val connection = HopsworksConnection . builder (). build (); val fs = connection . getFeatureStore (); Connecting from Databricks TBD Connecting from AWS SageMaker The Scala client version of hsfs is not supported on AWS SageMaker, please use the Python client.","title":"Examples"},{"location":"generated/project/#sharing-a-feature-store","text":"Connections are on a project-level, however, it is possible to share feature stores among projects, so even if you have a connection to one project, you can retireve a handle to any feature store shared with that project. To share a feature store, you can follow these steps: Sharing a Feature Store Open the project of the feature store that you would like to share on Hopsworks. Go to the Data Set browser and right click the Featurestore.db entry. Click Share with , then select Project and choose the project you wish to share the feature store with. Select the permissions level that the project user members should have on the feature store and click Share . Open the project you just shared the feature store with. Go to the Data Sets browser and there you should see the shared feature store as [project_name_of_shared_feature_store]::Featurestore.db . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this feature store from the other project. Sharing a feature store between projects Accepting a shared feature store from a project","title":"Sharing a Feature Store"},{"location":"generated/project/#connection-handle","text":"[source]","title":"Connection Handle"},{"location":"generated/project/#connection","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/project/#methods","text":"[source]","title":"Methods"},{"location":"generated/project/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/project/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/project/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source]","title":"get_feature_store"},{"location":"generated/project/#setup_databricks","text":"Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"setup_databricks"},{"location":"generated/query_vs_dataframe/","text":"Query vs DataFrame # HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters and point in time queries. To enable this functionality, we are introducing a new expressive Query abstraction with HSFS that provides these operations and guarantees reproducible creation of training datasets from features in the Feature Store. The new joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d ) # materialize query in the specified file format td . save ( feature_join ) # use materialized training dataset for training, possibly in a different environment td = fs . get_training_dataset ( \u201c rain_dataset \u201d , version = 1 ) # get TFRecordDataset to use in a TensorFlow model dataset = td . tf_data () . tf_record_dataset ( batch_size = 32 , num_epochs = 100 ) # reproduce query for online feature store and drop label for inference jdbc_querystring = td . get_query ( online = True , with_label = False ) Scala # create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . build ()) # materialize query in the specified file format td . save ( featureJoin ) # use materialized training dataset for training , possibly in a different environment val td = fs . getTrainingDataset ( \u201c rain_dataset \u201d , 1 ) # reproduce query for online feature store and drop label for inference val jdbcQuerystring = td . getQuery ( true , false ) If a data scientist wants to modify a new feature that is not available in the Feature Store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the Feature Store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources. The Query Abstraction # Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation. Examples # Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) Scala val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" )) Join # Similarly joins return queries. The simplest join, is one of two feature groups without specifying a join key or type. By default Hopsworks will use the maximal matching subset of the primary key of the two feature groups as joining key, if not specified otherwise. Python # Returns Query feature_join = rain_fg . join ( temperature_fg ) Scala # Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joines feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the name of the features to join on. Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure. Filter # In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) Scala val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ))) Methods # [source] as_of # Query . as_of ( wallclock_time ) [source] filter # Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] join # Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". Returns Query : A new Query object representing the join. [source] json # Query . json () [source] pull_changes # Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source] read # Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source] show # Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source] to_dict # Query . to_dict () [source] to_string # Query . to_string ( online = False ) Properties # [source] left_feature_group_end_time # [source] left_feature_group_start_time #","title":"Query vs. Dataframe"},{"location":"generated/query_vs_dataframe/#query-vs-dataframe","text":"HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters and point in time queries. To enable this functionality, we are introducing a new expressive Query abstraction with HSFS that provides these operations and guarantees reproducible creation of training datasets from features in the Feature Store. The new joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d ) # materialize query in the specified file format td . save ( feature_join ) # use materialized training dataset for training, possibly in a different environment td = fs . get_training_dataset ( \u201c rain_dataset \u201d , version = 1 ) # get TFRecordDataset to use in a TensorFlow model dataset = td . tf_data () . tf_record_dataset ( batch_size = 32 , num_epochs = 100 ) # reproduce query for online feature store and drop label for inference jdbc_querystring = td . get_query ( online = True , with_label = False ) Scala # create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . build ()) # materialize query in the specified file format td . save ( featureJoin ) # use materialized training dataset for training , possibly in a different environment val td = fs . getTrainingDataset ( \u201c rain_dataset \u201d , 1 ) # reproduce query for online feature store and drop label for inference val jdbcQuerystring = td . getQuery ( true , false ) If a data scientist wants to modify a new feature that is not available in the Feature Store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the Feature Store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources.","title":"Query vs DataFrame"},{"location":"generated/query_vs_dataframe/#the-query-abstraction","text":"Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation.","title":"The Query Abstraction"},{"location":"generated/query_vs_dataframe/#examples","text":"Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) Scala val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" ))","title":"Examples"},{"location":"generated/query_vs_dataframe/#join","text":"Similarly joins return queries. The simplest join, is one of two feature groups without specifying a join key or type. By default Hopsworks will use the maximal matching subset of the primary key of the two feature groups as joining key, if not specified otherwise. Python # Returns Query feature_join = rain_fg . join ( temperature_fg ) Scala # Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joines feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the name of the features to join on. Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure.","title":"Join"},{"location":"generated/query_vs_dataframe/#filter","text":"In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) Scala val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )))","title":"Filter"},{"location":"generated/query_vs_dataframe/#methods","text":"[source]","title":"Methods"},{"location":"generated/query_vs_dataframe/#as_of","text":"Query . as_of ( wallclock_time ) [source]","title":"as_of"},{"location":"generated/query_vs_dataframe/#filter_1","text":"Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/query_vs_dataframe/#join_1","text":"Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". Returns Query : A new Query object representing the join. [source]","title":"join"},{"location":"generated/query_vs_dataframe/#json","text":"Query . json () [source]","title":"json"},{"location":"generated/query_vs_dataframe/#pull_changes","text":"Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source]","title":"pull_changes"},{"location":"generated/query_vs_dataframe/#read","text":"Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source]","title":"read"},{"location":"generated/query_vs_dataframe/#show","text":"Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source]","title":"show"},{"location":"generated/query_vs_dataframe/#to_dict","text":"Query . to_dict () [source]","title":"to_dict"},{"location":"generated/query_vs_dataframe/#to_string","text":"Query . to_string ( online = False )","title":"to_string"},{"location":"generated/query_vs_dataframe/#properties","text":"[source]","title":"Properties"},{"location":"generated/query_vs_dataframe/#left_feature_group_end_time","text":"[source]","title":"left_feature_group_end_time"},{"location":"generated/query_vs_dataframe/#left_feature_group_start_time","text":"","title":"left_feature_group_start_time"},{"location":"generated/statistics/","text":"Statistics # HSFS provides functionality to compute statistics for training datasets and feature groups and save these along with their other metadata in the feature store . These statistics are meant to be helpful for Data Scientists to perform explorative data analysis and then recognize suitable features or training datasets for models. Statistics are configured on a training dataset or feature group level using a StatisticsConfig object. This object can be passed at creation time of the dataset or group or it can later on be updated through the API. [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] ) For example, to enable all statistics (descriptive, histograms and correlations) for a training dataset: Python from hsfs.statistics_config import StatisticsConfig td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d , statistics_config = StatisticsConfig ( true , true , true )) Scala val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . statisticsConfig ( new StatisticsConfig ( true , true , true )) . build ()) And similarly for feature groups. Default StatisticsConfig By default all training datasets and feature groups will be configured such that only descriptive statistics are computed. However, you can also enable histograms and correlations or limit the features for which statistics are computed. Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"Statistics"},{"location":"generated/statistics/#statistics","text":"HSFS provides functionality to compute statistics for training datasets and feature groups and save these along with their other metadata in the feature store . These statistics are meant to be helpful for Data Scientists to perform explorative data analysis and then recognize suitable features or training datasets for models. Statistics are configured on a training dataset or feature group level using a StatisticsConfig object. This object can be passed at creation time of the dataset or group or it can later on be updated through the API. [source]","title":"Statistics"},{"location":"generated/statistics/#statisticsconfig","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] ) For example, to enable all statistics (descriptive, histograms and correlations) for a training dataset: Python from hsfs.statistics_config import StatisticsConfig td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d , statistics_config = StatisticsConfig ( true , true , true )) Scala val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . statisticsConfig ( new StatisticsConfig ( true , true , true )) . build ()) And similarly for feature groups. Default StatisticsConfig By default all training datasets and feature groups will be configured such that only descriptive statistics are computed. However, you can also enable histograms and correlations or limit the features for which statistics are computed.","title":"StatisticsConfig"},{"location":"generated/statistics/#properties","text":"[source]","title":"Properties"},{"location":"generated/statistics/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/statistics/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/statistics/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/statistics/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/storage_connector/","text":"Storage Connector # Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. Properties # [source] access_key # Access key. [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] arguments # Additional JDBC arguments. [source] auto_create # Database username for redshift cluster. [source] bucket # Return the bucket for S3 connectors. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] connection_string # JDBC connection string. [source] connector_type # Type of the connector. S3, JDBC, REDSHIFT or HOPSFS. [source] container_name # Container name of the ADLS storage connector [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] directory_id # Directory ID of the ADLS storage connector [source] expiration # Cluster temporary credential expiration time. [source] generation # Generation of the ADLS storage connector [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] service_credential # Service credential of the ADLS storage connector [source] session_token # Session token. [source] table_name # Table name for redshift cluster. Methods # [source] spark_options # StorageConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # StorageConnector . to_dict ()","title":"Storage Connector"},{"location":"generated/storage_connector/#storage-connector","text":"Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.","title":"Storage Connector"},{"location":"generated/storage_connector/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/storage_connector/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/storage_connector/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/storage_connector/#properties","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/storage_connector/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/storage_connector/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/storage_connector/#arguments","text":"Additional JDBC arguments. [source]","title":"arguments"},{"location":"generated/storage_connector/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/storage_connector/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/storage_connector/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/storage_connector/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/storage_connector/#connector_type","text":"Type of the connector. S3, JDBC, REDSHIFT or HOPSFS. [source]","title":"connector_type"},{"location":"generated/storage_connector/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/storage_connector/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/storage_connector/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/storage_connector/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/storage_connector/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/storage_connector/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/storage_connector/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/storage_connector/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/storage_connector/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/storage_connector/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/storage_connector/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/storage_connector/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/storage_connector/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/storage_connector/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/storage_connector/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/storage_connector/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/storage_connector/#service_credential","text":"Service credential of the ADLS storage connector [source]","title":"service_credential"},{"location":"generated/storage_connector/#session_token","text":"Session token. [source]","title":"session_token"},{"location":"generated/storage_connector/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/storage_connector/#methods","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#spark_options","text":"StorageConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict","text":"StorageConnector . to_dict ()","title":"to_dict"},{"location":"generated/training_dataset/","text":"Training Dataset # The training dataset abstraction in Hopsworks Feature Store allows users to group a set of features (potentially from multiple different feature groups) with labels for training a model to do a particular prediction task. The training dataset is a versioned and managed dataset and is stored in HopsFS as tfrecords , parquet , csv , or tsv files. Versioning # Training Dataset can be versioned. Data Scientist should use the version to indicate to the model, as well as to the schema or the feature engineering logic of the features associated to this training dataset. Creation # To create training dataset, the user supplies a Pandas, Numpy or Spark dataframe with features and labels together with metadata. Once the training dataset has been created, the dataset is discoverable in the feature registry and users can use it to train models. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. Tagging Training Datasets # The feature store enables users to attach tags to training dataset in order to make them discoverable across feature stores. A tag is a simple {key: value} association, providing additional information about the data, such as for example geographic origin. This is useful in an organization as it makes easier to discover for data scientists, reduces duplicated work in terms of for example data preparation. The tagging feature is only available in the enterprise version. Define tags that can be attached # The first step is to define a set of tags that can be attached. Such as for example \u201cCountry\u201d to tag data as being from a certain geographic location and \u201cSport\u201d to further associate a type of Sport with the data. Attach tags using the UI # Tags can then be attached using the feature store UI or programmatically using the API. Attaching tags to feature group. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] data_format # File format of the training dataset. [source] description # [source] feature_store_id # [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get the latest computed statistics for the training dataset. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value = None ) Attach a name/value tag to a training dataset. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . [source] compute_statistics # TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag from a training dataset. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_statistics # TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source] get_tag # TrainingDataset . get_tag ( name = None ) Get the tags of a training dataset. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name : Name of the tag to get, defaults to None . Returns List[Tag] . List of tags as name/value pairs. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] tf_data # TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError . TFData engine # [source] tf_record_dataset # TFDataEngine . tf_record_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False , serialized_ndarray_fname = [], ) Reads tfrecord files and returns ParallelMapDataset or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object ParallelMapDataset can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 3 ) td . tf_data ( target_name = \"id\" ) . tf_record_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set True then one hot encode labels, defaults to False . num_classes Optional[int] : If above True then provide number of target classes, defaults to None . process Optional[bool] : If set True api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname Optional[list] : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . ParallelMapDataset . If process is set to False . [source] tf_csv_dataset # TFDataEngine . tf_csv_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False ) Reads csv files and returns CsvDatasetV2 or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object CsvDatasetV2 can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 1 ) td . tf_data ( target_name = \"id\" ) . tf_csv_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set true then one hot encode labels, defaults to False . num_classes Optional[int] : If above true then provide number of target classes, defaults to None . process Optional[bool] : If set true api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . CsvDatasetV2 . If process is set to False .","title":"Training Dataset"},{"location":"generated/training_dataset/#training-dataset","text":"The training dataset abstraction in Hopsworks Feature Store allows users to group a set of features (potentially from multiple different feature groups) with labels for training a model to do a particular prediction task. The training dataset is a versioned and managed dataset and is stored in HopsFS as tfrecords , parquet , csv , or tsv files.","title":"Training Dataset"},{"location":"generated/training_dataset/#versioning","text":"Training Dataset can be versioned. Data Scientist should use the version to indicate to the model, as well as to the schema or the feature engineering logic of the features associated to this training dataset.","title":"Versioning"},{"location":"generated/training_dataset/#creation","text":"To create training dataset, the user supplies a Pandas, Numpy or Spark dataframe with features and labels together with metadata. Once the training dataset has been created, the dataset is discoverable in the feature registry and users can use it to train models. [source]","title":"Creation"},{"location":"generated/training_dataset/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/training_dataset/#tagging-training-datasets","text":"The feature store enables users to attach tags to training dataset in order to make them discoverable across feature stores. A tag is a simple {key: value} association, providing additional information about the data, such as for example geographic origin. This is useful in an organization as it makes easier to discover for data scientists, reduces duplicated work in terms of for example data preparation. The tagging feature is only available in the enterprise version.","title":"Tagging Training Datasets"},{"location":"generated/training_dataset/#define-tags-that-can-be-attached","text":"The first step is to define a set of tags that can be attached. Such as for example \u201cCountry\u201d to tag data as being from a certain geographic location and \u201cSport\u201d to further associate a type of Sport with the data.","title":"Define tags that can be attached"},{"location":"generated/training_dataset/#attach-tags-using-the-ui","text":"Tags can then be attached using the feature store UI or programmatically using the API. Attaching tags to feature group.","title":"Attach tags using the UI"},{"location":"generated/training_dataset/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/training_dataset/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/training_dataset/#properties","text":"[source]","title":"Properties"},{"location":"generated/training_dataset/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/training_dataset/#description","text":"[source]","title":"description"},{"location":"generated/training_dataset/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/training_dataset/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/training_dataset/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/training_dataset/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/training_dataset/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/training_dataset/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/training_dataset/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/training_dataset/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/training_dataset/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/training_dataset/#statistics","text":"Get the latest computed statistics for the training dataset. [source]","title":"statistics"},{"location":"generated/training_dataset/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/training_dataset/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/training_dataset/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/training_dataset/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/training_dataset/#methods","text":"[source]","title":"Methods"},{"location":"generated/training_dataset/#add_tag","text":"TrainingDataset . add_tag ( name , value = None ) Attach a name/value tag to a training dataset. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . [source]","title":"add_tag"},{"location":"generated/training_dataset/#compute_statistics","text":"TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/training_dataset/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag from a training dataset. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. [source]","title":"delete_tag"},{"location":"generated/training_dataset/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/training_dataset/#get_statistics","text":"TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source]","title":"get_statistics"},{"location":"generated/training_dataset/#get_tag","text":"TrainingDataset . get_tag ( name = None ) Get the tags of a training dataset. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name : Name of the tag to get, defaults to None . Returns List[Tag] . List of tags as name/value pairs. [source]","title":"get_tag"},{"location":"generated/training_dataset/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/training_dataset/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/training_dataset/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/training_dataset/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/training_dataset/#tf_data","text":"TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source]","title":"tf_data"},{"location":"generated/training_dataset/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/training_dataset/#tfdata-engine","text":"[source]","title":"TFData engine"},{"location":"generated/training_dataset/#tf_record_dataset","text":"TFDataEngine . tf_record_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False , serialized_ndarray_fname = [], ) Reads tfrecord files and returns ParallelMapDataset or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object ParallelMapDataset can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 3 ) td . tf_data ( target_name = \"id\" ) . tf_record_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set True then one hot encode labels, defaults to False . num_classes Optional[int] : If above True then provide number of target classes, defaults to None . process Optional[bool] : If set True api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname Optional[list] : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . ParallelMapDataset . If process is set to False . [source]","title":"tf_record_dataset"},{"location":"generated/training_dataset/#tf_csv_dataset","text":"TFDataEngine . tf_csv_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False ) Reads csv files and returns CsvDatasetV2 or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object CsvDatasetV2 can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 1 ) td . tf_data ( target_name = \"id\" ) . tf_csv_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set true then one hot encode labels, defaults to False . num_classes Optional[int] : If above true then provide number of target classes, defaults to None . process Optional[bool] : If set true api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . CsvDatasetV2 . If process is set to False .","title":"tf_csv_dataset"},{"location":"generated/api/connection_api/","text":"Connection # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] cert_folder # [source] host # [source] hostname_verification # [source] port # [source] project # [source] region_name # [source] secrets_store # [source] trust_store_path # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source] setup_databricks # Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"Connection"},{"location":"generated/api/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/api/connection_api/#connection_1","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/api/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/api/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/api/connection_api/#cert_folder","text":"[source]","title":"cert_folder"},{"location":"generated/api/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/api/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/api/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/api/connection_api/#project","text":"[source]","title":"project"},{"location":"generated/api/connection_api/#region_name","text":"[source]","title":"region_name"},{"location":"generated/api/connection_api/#secrets_store","text":"[source]","title":"secrets_store"},{"location":"generated/api/connection_api/#trust_store_path","text":"","title":"trust_store_path"},{"location":"generated/api/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/api/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/api/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source]","title":"connection"},{"location":"generated/api/connection_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source]","title":"get_feature_store"},{"location":"generated/api/connection_api/#setup_databricks","text":"Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"setup_databricks"},{"location":"generated/api/feature_api/","text":"Feature # [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] from_response_json # Feature . from_response_json ( json_dict ) [source] json # Feature . json () [source] to_dict # Feature . to_dict ()","title":"Feature"},{"location":"generated/api/feature_api/#feature","text":"[source]","title":"Feature"},{"location":"generated/api/feature_api/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/api/feature_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_api/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/api/feature_api/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_api/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/api/feature_api/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/api/feature_api/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/api/feature_api/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/api/feature_api/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/api/feature_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_api/#from_response_json","text":"Feature . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_api/#json","text":"Feature . json () [source]","title":"json"},{"location":"generated/api/feature_api/#to_dict","text":"Feature . to_dict ()","title":"to_dict"},{"location":"generated/api/feature_group_api/","text":"FeatureGroup # [source] FeatureGroup # hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , jobs = None , online_enabled = False , time_travel_format = None , statistics_config = None , ) Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] description # Description of the feature group contents. [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Schema information. [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] name # Name of the feature group. [source] online_enabled # Setting if the feature group is available in online storage. [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] time_travel_format # Setting of the feature group time travel format. [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value = None ) Attach a name/value tag to a feature group. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . Raises RestAPIError . [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source] commit_details # FeatureGroup . commit_details ( limit = None ) Retrieves commit timeline for this feature group. Arguments limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . [source] compute_statistics # FeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag from a feature group. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. Raises RestAPIError . [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_response_json # FeatureGroup . from_response_json ( json_dict ) [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name = None ) Get the tags of a feature group. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name Optional[str] : Name of the tag to get, defaults to None . Returns list[Tag] . List of tags as name/value pairs. Raises RestAPIError . [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Updated feature group metadata object. [source] json # FeatureGroup . json () [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature group's with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. [source] save # FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Returns the persisted FeatureGroup metadata object. Raises RestAPIError . Unable to create feature group. [source] select # FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] to_dict # FeatureGroup . to_dict () [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source] update_from_response_json # FeatureGroup . update_from_response_json ( json_dict ) [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError .","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup","text":"[source]","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup_1","text":"hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , jobs = None , online_enabled = False , time_travel_format = None , statistics_config = None , )","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/feature_group_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object.","title":"create_feature_group"},{"location":"generated/api/feature_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_group_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/api/feature_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_group_api/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/api/feature_group_api/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/api/feature_group_api/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/api/feature_group_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/feature_group_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/feature_group_api/#features","text":"Schema information. [source]","title":"features"},{"location":"generated/api/feature_group_api/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_group_api/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/api/feature_group_api/#location","text":"[source]","title":"location"},{"location":"generated/api/feature_group_api/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/api/feature_group_api/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/api/feature_group_api/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/api/feature_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/feature_group_api/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/api/feature_group_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/api/feature_group_api/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/api/feature_group_api/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/api/feature_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_group_api/#add_tag","text":"FeatureGroup . add_tag ( name , value = None ) Attach a name/value tag to a feature group. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . Raises RestAPIError . [source]","title":"add_tag"},{"location":"generated/api/feature_group_api/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source]","title":"append_features"},{"location":"generated/api/feature_group_api/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/api/feature_group_api/#commit_details","text":"FeatureGroup . commit_details ( limit = None ) Retrieves commit timeline for this feature group. Arguments limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . [source]","title":"commit_details"},{"location":"generated/api/feature_group_api/#compute_statistics","text":"FeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/api/feature_group_api/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/feature_group_api/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag from a feature group. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. Raises RestAPIError . [source]","title":"delete_tag"},{"location":"generated/api/feature_group_api/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/feature_group_api/#from_response_json","text":"FeatureGroup . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_group_api/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/api/feature_group_api/#get_statistics","text":"FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/api/feature_group_api/#get_tag","text":"FeatureGroup . get_tag ( name = None ) Get the tags of a feature group. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name Optional[str] : Name of the tag to get, defaults to None . Returns list[Tag] . List of tags as name/value pairs. Raises RestAPIError . [source]","title":"get_tag"},{"location":"generated/api/feature_group_api/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Updated feature group metadata object. [source]","title":"insert"},{"location":"generated/api/feature_group_api/#json","text":"FeatureGroup . json () [source]","title":"json"},{"location":"generated/api/feature_group_api/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/api/feature_group_api/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature group's with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. [source]","title":"read_changes"},{"location":"generated/api/feature_group_api/#save","text":"FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs, defaults to {} . Returns FeatureGroup . Returns the persisted FeatureGroup metadata object. Raises RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/api/feature_group_api/#select","text":"FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/feature_group_api/#select_all","text":"FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/feature_group_api/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/feature_group_api/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/api/feature_group_api/#to_dict","text":"FeatureGroup . to_dict () [source]","title":"to_dict"},{"location":"generated/api/feature_group_api/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/feature_group_api/#update_from_response_json","text":"FeatureGroup . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/feature_group_api/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/api/feature_store_api/","text":"Feature Store # [source] FeatureStore # hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , ) Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] description # Description of the feature store. [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source] from_response_json # FeatureStore . from_response_json ( json_dict ) [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False )","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#feature-store","text":"[source]","title":"Feature Store"},{"location":"generated/api/feature_store_api/#featurestore","text":"hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , )","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_store_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/api/feature_store_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_store_api/#description","text":"Description of the feature store. [source]","title":"description"},{"location":"generated/api/feature_store_api/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/api/feature_store_api/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/api/feature_store_api/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/api/feature_store_api/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/api/feature_store_api/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/api/feature_store_api/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/api/feature_store_api/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/api/feature_store_api/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/api/feature_store_api/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/api/feature_store_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_store_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/api/feature_store_api/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/api/feature_store_api/#from_response_json","text":"FeatureStore . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_store_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/api/feature_store_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/feature_store_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/api/feature_store_api/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False )","title":"sql"},{"location":"generated/api/statistics_config_api/","text":"StatisticsConfig # [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] ) Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig","text":"[source]","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig_1","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] )","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/statistics_config_api/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/api/statistics_config_api/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/api/statistics_config_api/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/api/statistics_config_api/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/api/storage_connector_api/","text":"Storage Connector # Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. Properties # [source] access_key # Access key. [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] arguments # Additional JDBC arguments. [source] auto_create # Database username for redshift cluster. [source] bucket # Return the bucket for S3 connectors. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] connection_string # JDBC connection string. [source] connector_type # Type of the connector. S3, JDBC, REDSHIFT or HOPSFS. [source] container_name # Container name of the ADLS storage connector [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] directory_id # Directory ID of the ADLS storage connector [source] expiration # Cluster temporary credential expiration time. [source] generation # Generation of the ADLS storage connector [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] service_credential # Service credential of the ADLS storage connector [source] session_token # Session token. [source] table_name # Table name for redshift cluster. Methods # [source] from_response_json # StorageConnector . from_response_json ( json_dict ) [source] spark_options # StorageConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # StorageConnector . to_dict ()","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#storage-connector","text":"","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/storage_connector_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/storage_connector_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/api/storage_connector_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/api/storage_connector_api/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/api/storage_connector_api/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/api/storage_connector_api/#arguments","text":"Additional JDBC arguments. [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/api/storage_connector_api/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/api/storage_connector_api/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/api/storage_connector_api/#connector_type","text":"Type of the connector. S3, JDBC, REDSHIFT or HOPSFS. [source]","title":"connector_type"},{"location":"generated/api/storage_connector_api/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/api/storage_connector_api/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/api/storage_connector_api/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/api/storage_connector_api/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/api/storage_connector_api/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/api/storage_connector_api/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/api/storage_connector_api/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/api/storage_connector_api/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/api/storage_connector_api/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/api/storage_connector_api/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/api/storage_connector_api/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/api/storage_connector_api/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/api/storage_connector_api/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/api/storage_connector_api/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/api/storage_connector_api/#service_credential","text":"Service credential of the ADLS storage connector [source]","title":"service_credential"},{"location":"generated/api/storage_connector_api/#session_token","text":"Session token. [source]","title":"session_token"},{"location":"generated/api/storage_connector_api/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/api/storage_connector_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#from_response_json","text":"StorageConnector . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/storage_connector_api/#spark_options","text":"StorageConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict","text":"StorageConnector . to_dict ()","title":"to_dict"},{"location":"generated/api/training_dataset_api/","text":"Training Dataset # [source] TrainingDataset # hsfs . training_dataset . TrainingDataset ( name , version , data_format , location , featurestore_id , description = None , storage_connector = None , splits = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , jobs = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , ) Creation # [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] data_format # File format of the training dataset. [source] description # [source] feature_store_id # [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get the latest computed statistics for the training dataset. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value = None ) Attach a name/value tag to a training dataset. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . [source] compute_statistics # TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag from a training dataset. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. [source] from_response_json # TrainingDataset . from_response_json ( json_dict ) [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_statistics # TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source] get_tag # TrainingDataset . get_tag ( name = None ) Get the tags of a training dataset. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name : Name of the tag to get, defaults to None . Returns List[Tag] . List of tags as name/value pairs. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source] json # TrainingDataset . json () [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] tf_data # TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source] to_dict # TrainingDataset . to_dict () [source] update_from_response_json # TrainingDataset . update_from_response_json ( json_dict ) [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#training-dataset","text":"[source]","title":"Training Dataset"},{"location":"generated/api/training_dataset_api/#trainingdataset","text":"hsfs . training_dataset . TrainingDataset ( name , version , data_format , location , featurestore_id , description = None , storage_connector = None , splits = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , jobs = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , )","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/training_dataset_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/api/training_dataset_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/training_dataset_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/api/training_dataset_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/training_dataset_api/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/api/training_dataset_api/#description","text":"[source]","title":"description"},{"location":"generated/api/training_dataset_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/training_dataset_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/training_dataset_api/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/api/training_dataset_api/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/api/training_dataset_api/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/api/training_dataset_api/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/api/training_dataset_api/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/api/training_dataset_api/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/api/training_dataset_api/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/api/training_dataset_api/#statistics","text":"Get the latest computed statistics for the training dataset. [source]","title":"statistics"},{"location":"generated/api/training_dataset_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/api/training_dataset_api/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/api/training_dataset_api/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/api/training_dataset_api/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/api/training_dataset_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/training_dataset_api/#add_tag","text":"TrainingDataset . add_tag ( name , value = None ) Attach a name/value tag to a training dataset. A tag can consist of a name only or a name/value pair. Tag names are unique identifiers. Arguments name str : Name of the tag to be added. value Optional[str] : Value of the tag to be added, defaults to None . [source]","title":"add_tag"},{"location":"generated/api/training_dataset_api/#compute_statistics","text":"TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/api/training_dataset_api/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag from a training dataset. Tag names are unique identifiers. Arguments name str : Name of the tag to be removed. [source]","title":"delete_tag"},{"location":"generated/api/training_dataset_api/#from_response_json","text":"TrainingDataset . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/training_dataset_api/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/api/training_dataset_api/#get_statistics","text":"TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source]","title":"get_statistics"},{"location":"generated/api/training_dataset_api/#get_tag","text":"TrainingDataset . get_tag ( name = None ) Get the tags of a training dataset. Tag names are unique identifiers. Returns all tags if no tag name is specified. Arguments name : Name of the tag to get, defaults to None . Returns List[Tag] . List of tags as name/value pairs. [source]","title":"get_tag"},{"location":"generated/api/training_dataset_api/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/api/training_dataset_api/#json","text":"TrainingDataset . json () [source]","title":"json"},{"location":"generated/api/training_dataset_api/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/api/training_dataset_api/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key/value pairs. Defaults to {} . Returns TrainingDataset : The updated training dataset metadata object, the previous TrainingDataset object on which you call save is also updated. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/api/training_dataset_api/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/api/training_dataset_api/#tf_data","text":"TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source]","title":"tf_data"},{"location":"generated/api/training_dataset_api/#to_dict","text":"TrainingDataset . to_dict () [source]","title":"to_dict"},{"location":"generated/api/training_dataset_api/#update_from_response_json","text":"TrainingDataset . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"update_statistics_config"},{"location":"hopsworksai/","text":"Hopsworks.ai # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker, and KubeFlow. Getting started # To get started with deploying a cluster in you Azure or AWS environment, follow these guides: Azure , AWS . If you need more details about hopsworks.ai cluster creation steps you can check the following guides: Azure , AWS . Limiting permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s cloud account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. The following guides explain how you can reduce the permissions you give to hopsworks.ai by doing some of the cluster creation steps manually: Azure , AWS . Integration with managed Kubernetes # When deploying a cluster you can set it up to use managed Kubernetes to run python jobs, Jupyter servers, and ML model serving in a scalable way. This guide provides step-by-step instructions on how to set up Kubernetes and start a cluster using it: AWS . Integration with third-party platforms # Once you have deployed a cluster with hopsworks.ai you can connect to it from third-party platforms such as Databricks , AWS Sagemaker , Azure HDInsight , etc. Go to integrations for more third-party platforms and details. Other # You can find more information on the following topics by clicking on the links: GPU support Ading and removing workers User management","title":"Introduction"},{"location":"hopsworksai/#hopsworksai","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker, and KubeFlow.","title":"Hopsworks.ai"},{"location":"hopsworksai/#getting-started","text":"To get started with deploying a cluster in you Azure or AWS environment, follow these guides: Azure , AWS . If you need more details about hopsworks.ai cluster creation steps you can check the following guides: Azure , AWS .","title":"Getting started"},{"location":"hopsworksai/#limiting-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s cloud account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. The following guides explain how you can reduce the permissions you give to hopsworks.ai by doing some of the cluster creation steps manually: Azure , AWS .","title":"Limiting permissions"},{"location":"hopsworksai/#integration-with-managed-kubernetes","text":"When deploying a cluster you can set it up to use managed Kubernetes to run python jobs, Jupyter servers, and ML model serving in a scalable way. This guide provides step-by-step instructions on how to set up Kubernetes and start a cluster using it: AWS .","title":"Integration with managed Kubernetes"},{"location":"hopsworksai/#integration-with-third-party-platforms","text":"Once you have deployed a cluster with hopsworks.ai you can connect to it from third-party platforms such as Databricks , AWS Sagemaker , Azure HDInsight , etc. Go to integrations for more third-party platforms and details.","title":"Integration with third-party platforms"},{"location":"hopsworksai/#other","text":"You can find more information on the following topics by clicking on the links: GPU support Ading and removing workers User management","title":"Other"},{"location":"hopsworksai/adding_removing_workers/","text":"Adding and removing workers # Once you have started a hopsworks cluster you can add and remove workers from the cluster to accommodate your workload. Adding workers # If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting. Removing workers # If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers Hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Adding and Removing workers"},{"location":"hopsworksai/adding_removing_workers/#adding-and-removing-workers","text":"Once you have started a hopsworks cluster you can add and remove workers from the cluster to accommodate your workload.","title":"Adding and removing workers"},{"location":"hopsworksai/adding_removing_workers/#adding-workers","text":"If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting.","title":"Adding workers"},{"location":"hopsworksai/adding_removing_workers/#removing-workers","text":"If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers Hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Removing workers"},{"location":"hopsworksai/gpu_support/","text":"GPU support # Hopsworks can harness the power of GPUs to speed up machine learning processes. You can take advantage of this feature in Hopsworks.ai by adding GPU equipped workers to your cluster. This can be done in two way: creating a cluster with GPU equipped workers or adding GPU equipped workers to an existing cluster. Creating a cluster with GPU equipped workers # When selecting the workers' instance type during the cluster creation, you can select an instance type equipped with GPUs. The cluster will then be created and Hopsworks will automatically detect the GPU resource. Create cluster with GPUs Adding GPU equipped workers to an existing cluster. # When adding workers to a cluster, you can select an instance type equipped with GPUs. The workers will then be added to the cluster and Hopsworks will automatically detect the new GPU resource. Add GPUs to cluster Using the GPUs # Once workers with GPUs have been added to your cluster you can use them by allocating GPUs to JupyterLab or Jobs. Using GPUs in JupyterLab Using GPUs in jobs For more information about using GPUs in Hopsworks you can consult Hopsworks Experiments documentation .","title":"GPU support"},{"location":"hopsworksai/gpu_support/#gpu-support","text":"Hopsworks can harness the power of GPUs to speed up machine learning processes. You can take advantage of this feature in Hopsworks.ai by adding GPU equipped workers to your cluster. This can be done in two way: creating a cluster with GPU equipped workers or adding GPU equipped workers to an existing cluster.","title":"GPU support"},{"location":"hopsworksai/gpu_support/#creating-a-cluster-with-gpu-equipped-workers","text":"When selecting the workers' instance type during the cluster creation, you can select an instance type equipped with GPUs. The cluster will then be created and Hopsworks will automatically detect the GPU resource. Create cluster with GPUs","title":"Creating a cluster with GPU equipped workers"},{"location":"hopsworksai/gpu_support/#adding-gpu-equipped-workers-to-an-existing-cluster","text":"When adding workers to a cluster, you can select an instance type equipped with GPUs. The workers will then be added to the cluster and Hopsworks will automatically detect the new GPU resource. Add GPUs to cluster","title":"Adding GPU equipped workers to an existing cluster."},{"location":"hopsworksai/gpu_support/#using-the-gpus","text":"Once workers with GPUs have been added to your cluster you can use them by allocating GPUs to JupyterLab or Jobs. Using GPUs in JupyterLab Using GPUs in jobs For more information about using GPUs in Hopsworks you can consult Hopsworks Experiments documentation .","title":"Using the GPUs"},{"location":"hopsworksai/user_management/","text":"User management # In Hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with Hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting. Adding members to an organization # Organization membership can be edited by clicking Members on the left of Hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. An invited user must accept the invitation to be part of the organization. An invitation will show up in the invited member's Dashboard. In this example Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard Sharing resources # Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters. Removing members from an organization # To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member","title":"User management"},{"location":"hopsworksai/user_management/#user-management","text":"In Hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with Hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting.","title":"User management"},{"location":"hopsworksai/user_management/#adding-members-to-an-organization","text":"Organization membership can be edited by clicking Members on the left of Hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. An invited user must accept the invitation to be part of the organization. An invitation will show up in the invited member's Dashboard. In this example Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard","title":"Adding members to an organization"},{"location":"hopsworksai/user_management/#sharing-resources","text":"Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters.","title":"Sharing resources"},{"location":"hopsworksai/user_management/#removing-members-from-an-organization","text":"To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member","title":"Removing members from an organization"},{"location":"hopsworksai/aws/cluster_creation/","text":"Getting started with Hopsworks.ai (AWS) # This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai Step 1 starting to create a cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Select the number of workers you want to start the cluster with (5). Select the Instance type (6) and Local storage size (7) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. Enter the name of the S3 bucket (8) you want the cluster to store its data in, in S3 bucket . Note The S3 bucket you are using must be empty. Press Next (9): Create a Hopsworks cluster, general information Step 3 select an SSH key # When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key Step 4 select the Instance Profile: # To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket Choose the instance profile Step 5 set the backup retention policy: # Note This step is only accessible to enterprise users. To back up the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 6 Managed Containers: # Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS and Amazon ECR . Add EKS cluster name Step 7 VPC selection # In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC Step 8 Availability Zone selection # If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone Step 9 Security group selection # If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note For Hopsworks.ai to create the SSL certificates the security group needs to allow inbound traffic on port 80. If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Choose security group Step 10 User management selection # In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 11 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 12 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"AWS"},{"location":"hopsworksai/aws/cluster_creation/#getting-started-with-hopsworksai-aws","text":"This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"hopsworksai/aws/cluster_creation/#step-2-setting-the-general-information","text":"Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Select the number of workers you want to start the cluster with (5). Select the Instance type (6) and Local storage size (7) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. Enter the name of the S3 bucket (8) you want the cluster to store its data in, in S3 bucket . Note The S3 bucket you are using must be empty. Press Next (9): Create a Hopsworks cluster, general information","title":"Step 2 setting the General information"},{"location":"hopsworksai/aws/cluster_creation/#step-3-select-an-ssh-key","text":"When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key","title":"Step 3 select an SSH key"},{"location":"hopsworksai/aws/cluster_creation/#step-4-select-the-instance-profile","text":"To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket Choose the instance profile","title":"Step 4 select the Instance Profile:"},{"location":"hopsworksai/aws/cluster_creation/#step-5-set-the-backup-retention-policy","text":"Note This step is only accessible to enterprise users. To back up the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 5 set the backup retention policy:"},{"location":"hopsworksai/aws/cluster_creation/#step-6-managed-containers","text":"Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS and Amazon ECR . Add EKS cluster name","title":"Step 6 Managed Containers:"},{"location":"hopsworksai/aws/cluster_creation/#step-7-vpc-selection","text":"In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC","title":"Step 7 VPC selection"},{"location":"hopsworksai/aws/cluster_creation/#step-8-availability-zone-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone","title":"Step 8 Availability Zone selection"},{"location":"hopsworksai/aws/cluster_creation/#step-9-security-group-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note For Hopsworks.ai to create the SSL certificates the security group needs to allow inbound traffic on port 80. If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Choose security group","title":"Step 9 Security group selection"},{"location":"hopsworksai/aws/cluster_creation/#step-10-user-management-selection","text":"In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 10 User management selection"},{"location":"hopsworksai/aws/cluster_creation/#step-11-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 11 add tags to your instances."},{"location":"hopsworksai/aws/cluster_creation/#step-12-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 12 Review and create"},{"location":"hopsworksai/aws/eks_ecr_integration/","text":"Integration with Amazon EKS and Amazon ECR # This guide shows how to create a cluster in Hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster. Step 1: Create an EKS cluster on AWS # If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command. Step 1.1: Installing eksctl, aws, and kubectl # Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl. Step 1.2: Create an EKS cluster using eksctl # You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.17 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .17 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .17 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976 Step 2: Create an instance profile role on AWS # You need to create an instance profile role to allow instances created by Hopsworks.ai to access EKS and ECR. To create a role, click on the following link . Alternatively, you can go to the Roles section of the IAM service in AWS management console, click on Create role , choose AWS Service as the type of trusted entity, and then choose EC2 from Common use cases. Then, click on Next: Permissions , Next: Tags , Next: Review , and then name your role and click Create role . Navigate to your newly created role in AWS management console by searching for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Finally, copy your Role ARN (you will need it in the next steps). { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowPullMainImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/filebeat\" , \"arn:aws:ecr:*:*:repository/base\" ] }, { \"Sid\" : \"AllowPushandPullImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:CreateRepository\" , \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:DeleteRepository\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" }, { \"Sid\" : \"HopsFSS3Permissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::bucket.name/*\" , \"arn:aws:s3:::bucket.name\" ] } ] } Replace BUCKET_NAME with the appropriate S3 bucket name. Non-enterprise users can remove the policies S3:PutLifecycleConfiguration , S3:GetLifecycleConfiguration , S3:PutBucketVersioning , S3:GetBucketVersioning as these policies are needed for cluster backups and restore operations available only for the enterprise version. Step 3: Allow your role to use your EKS cluster # You need to give your role permissions to access your EKS cluster using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . The kubectl edit command uses vi editor by default, however, you can override this behaviour by setting KUBE_EDITOR to your preferred editor, check Kubernetes editing resources . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with your role RoleARN before saving. Warning You need to use the RoleARN not the instance profile ARN, also make sure to keep the same formatting as in the example below. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save the updated config map. configmap/aws-auth edited Step 4: Open Hopsworks required ports on your EKS cluster security group # You need to open the HTTP (80) and HTTPS (443) ports on the security group of your EKS cluster. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to change the cluster name according to your setup in Step 1 or if you have an existing cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , that will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Once you get the security group id (YOUR_EKS_SECURITY_GROUP_ID), you need to proceed to the AWS management console by clicking on security groups . Filter security groups using the Security Group ID and then paste your EKS security group id. Click on the inbound rules tab, then click on the Edit inbound rules , now you should arrive at the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group. Step 5: Allow Hopsworks.ai to delete ECR repositories on your behalf # You need to add another inline policy to your role or user connected to Hopsworks.ai, see [../getting_started.md]. First, navigate to AWS management console , then click on Roles or Users depending on which connection method you have used in Hopsworks.ai, and then search for your role or user name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDeletingECRRepositories\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:DeleteRepository\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] } ] } Step 6: Create a Hopsworks cluster with EKS and ECR support # In Hopsworks.ai, select Create cluster . Choose the region of your EKS cluster and fill in the name of your S3 bucket, then click Next: Create Hopsworks cluster Choose your preferred SSH key to use with the cluster, then click Next: Choose SSH key Choose the instance profile role that you have created in Step 2, then click Next: Choose instance profile role Choose Enabled to enable the use of Amazon EKS and ECR: Choose Enabled Add your EKS cluster name and update your AWS account id if you want to use another account for ECR, then click Next: Add EKS cluster name Choose the VPC of your EKS cluster, then click Next: Choose VPC Choose any of the subnets in the VPC, then click Next: Choose Subnet Choose the security group that you have updated in Step 4, then click Next: Note Select the Security Group in the form of eks-cluster-sg-YOUR-CLUSTER-NAME-* and NOT the ones for ControlPlaneSecurity or ClusterSharedNode. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"Integration with EKS and ECR"},{"location":"hopsworksai/aws/eks_ecr_integration/#integration-with-amazon-eks-and-amazon-ecr","text":"This guide shows how to create a cluster in Hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster.","title":"Integration with Amazon EKS and Amazon ECR"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-1-create-an-eks-cluster-on-aws","text":"If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command.","title":"Step 1: Create an EKS cluster on AWS"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-11-installing-eksctl-aws-and-kubectl","text":"Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl.","title":"Step 1.1: Installing eksctl, aws, and kubectl"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-12-create-an-eks-cluster-using-eksctl","text":"You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.17 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .17 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .17 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976","title":"Step 1.2: Create an EKS cluster using eksctl"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-2-create-an-instance-profile-role-on-aws","text":"You need to create an instance profile role to allow instances created by Hopsworks.ai to access EKS and ECR. To create a role, click on the following link . Alternatively, you can go to the Roles section of the IAM service in AWS management console, click on Create role , choose AWS Service as the type of trusted entity, and then choose EC2 from Common use cases. Then, click on Next: Permissions , Next: Tags , Next: Review , and then name your role and click Create role . Navigate to your newly created role in AWS management console by searching for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Finally, copy your Role ARN (you will need it in the next steps). { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowPullMainImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/filebeat\" , \"arn:aws:ecr:*:*:repository/base\" ] }, { \"Sid\" : \"AllowPushandPullImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:CreateRepository\" , \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:DeleteRepository\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" }, { \"Sid\" : \"HopsFSS3Permissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::bucket.name/*\" , \"arn:aws:s3:::bucket.name\" ] } ] } Replace BUCKET_NAME with the appropriate S3 bucket name. Non-enterprise users can remove the policies S3:PutLifecycleConfiguration , S3:GetLifecycleConfiguration , S3:PutBucketVersioning , S3:GetBucketVersioning as these policies are needed for cluster backups and restore operations available only for the enterprise version.","title":"Step 2: Create an instance profile role on AWS"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-3-allow-your-role-to-use-your-eks-cluster","text":"You need to give your role permissions to access your EKS cluster using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . The kubectl edit command uses vi editor by default, however, you can override this behaviour by setting KUBE_EDITOR to your preferred editor, check Kubernetes editing resources . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with your role RoleARN before saving. Warning You need to use the RoleARN not the instance profile ARN, also make sure to keep the same formatting as in the example below. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save the updated config map. configmap/aws-auth edited","title":"Step 3: Allow your role to use your EKS cluster"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-4-open-hopsworks-required-ports-on-your-eks-cluster-security-group","text":"You need to open the HTTP (80) and HTTPS (443) ports on the security group of your EKS cluster. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to change the cluster name according to your setup in Step 1 or if you have an existing cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , that will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Once you get the security group id (YOUR_EKS_SECURITY_GROUP_ID), you need to proceed to the AWS management console by clicking on security groups . Filter security groups using the Security Group ID and then paste your EKS security group id. Click on the inbound rules tab, then click on the Edit inbound rules , now you should arrive at the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group.","title":"Step 4: Open Hopsworks required ports on your EKS cluster security group"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-5-allow-hopsworksai-to-delete-ecr-repositories-on-your-behalf","text":"You need to add another inline policy to your role or user connected to Hopsworks.ai, see [../getting_started.md]. First, navigate to AWS management console , then click on Roles or Users depending on which connection method you have used in Hopsworks.ai, and then search for your role or user name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDeletingECRRepositories\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:DeleteRepository\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] } ] }","title":"Step 5: Allow Hopsworks.ai to delete ECR repositories on your behalf"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-6-create-a-hopsworks-cluster-with-eks-and-ecr-support","text":"In Hopsworks.ai, select Create cluster . Choose the region of your EKS cluster and fill in the name of your S3 bucket, then click Next: Create Hopsworks cluster Choose your preferred SSH key to use with the cluster, then click Next: Choose SSH key Choose the instance profile role that you have created in Step 2, then click Next: Choose instance profile role Choose Enabled to enable the use of Amazon EKS and ECR: Choose Enabled Add your EKS cluster name and update your AWS account id if you want to use another account for ECR, then click Next: Add EKS cluster name Choose the VPC of your EKS cluster, then click Next: Choose VPC Choose any of the subnets in the VPC, then click Next: Choose Subnet Choose the security group that you have updated in Step 4, then click Next: Note Select the Security Group in the form of eks-cluster-sg-YOUR-CLUSTER-NAME-* and NOT the ones for ControlPlaneSecurity or ClusterSharedNode. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"Step 6: Create a Hopsworks cluster with EKS and ECR support"},{"location":"hopsworksai/aws/getting_started/","text":"Getting started with Hopsworks.ai (AWS) # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's AWS account. Step 1: Connecting your AWS account # Hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This can be either achieved by using AWS cross-account roles or AWS access keys. We strongly recommend the usage of cross-account roles whenever possible due to security reasons. Option 1: Using AWS Cross-Account Roles # To create a cross-account role for Hopsworks.ai, you need our AWS account id and the external id we created for you. You can find this information on the first screen of the cross-account configuration flow. Take note of the account id and external id and go to the Roles section of the IAM service in the AWS Management Console and select Create role . Creating the cross-account role instructions Select Another AWS account as trusted entity and fill in our AWS account id and the external id generated for you: Creating the cross-account role step 1 Go to the last step of the wizard, name the role and create it: Creating the cross-account role step 2 As a next step, you need to create an access policy to give Hopsworks.ai permissions to manage clusters in your organization's AWS account. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . Copy the permission JSON from the instructions: Adding the policy instructions Identify your newly created cross-account role in the Roles section of the IAM service in the AWS Management Console and select Add inline policy : Adding the inline policy step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 Copy the Role ARN from the summary of your cross-account role: Adding the inline policy step 4 Paste the Role ARN into Hopsworks.ai and click on Finish : Saving the cross-account role Option 2: Using AWS Access Keys # You can either create a new IAM user or use an existing IAM user to create access keys for Hopsworks.ai. If you want to create a new IAM user, see Creating an IAM User in Your AWS Account . Warning We recommend using Cross-Account Roles instead of Access Keys whenever possible, see Option 1: Using AWS Cross-Account Roles . Hopsworks.ai requires a set of permissions to be able to launch clusters in your AWS account. The permissions can be granted by attaching an access policy to your IAM user. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . The required permissions are shown in the instructions. Copy them if you want to create a new access policy: Configuring access key instructions Add a new Inline policy to your AWS user: Configuring the access key on AWS step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 In the overview of your IAM user, select Create access key : Configuring the access key on AWS step 2 Copy the Access Key ID and the Secret Access Key : Configuring the access key on AWS step 3 Paste the Access Key ID and the Secret Access Key into Hopsworks.ai and click on Finish : Saving the access key pair Step 2: Creating Instance profile # Hopsworks cluster nodes need access to certain resources such as S3 bucket and CloudWatch. Follow the instructions in this guide to create an IAM instance profile with access to your S3 bucket: Guide When creating the policy, paste the following in the JSON tab. Replace BUCKET_NAME with the appropriate S3 bucket name. Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } ] } Step 3: Creating storage # The Hopsworks clusters deployed by hopsworks.ai store their data in an S3 bucket in your AWS account. To enable this you need to create an S3 bucket and an instance profile to give cluster nodes access to the bucket. Proceed to the S3 Management Console and click on Create bucket : Create an S3 bucket Name your bucket and select the region where your Hopsworks cluster will run. Click on Create bucket at the bottom of the page. Create an S3 bucket Step 4: Create an SSH key # When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair . Step 4.1: Create a new key pair # Proceed to Key pairs in the EC2 console and click on Create key pair Create a key pair Name your key, select the file format you prefer and click on Create key pair . Create a key pair Step 4.2: Import a key pair # Proceed to Key pairs in the EC2 console , click on Action and click on Import key pair Import a key pair Name your key pair, upload your public key and click on Import key pair . Import a key pair Step 5: Deploying a Hopsworks cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Select the number of workers you want to start the cluster with (5). Select the Instance type (6) and Local storage size (7) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. Enter the name of the S3 bucket (8) you created above in S3 bucket . Note The S3 bucket you are using must be empty. Press Next (9): Create a Hopsworks cluster, general information Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the Instance Profile that you created above and click on Review and Submit : Choose the instance profile Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster Step 6: Outside Access to the Feature Store # By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store Step 7: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"AWS"},{"location":"hopsworksai/aws/getting_started/#getting-started-with-hopsworksai-aws","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's AWS account.","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/getting_started/#step-1-connecting-your-aws-account","text":"Hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This can be either achieved by using AWS cross-account roles or AWS access keys. We strongly recommend the usage of cross-account roles whenever possible due to security reasons.","title":"Step 1: Connecting your AWS account"},{"location":"hopsworksai/aws/getting_started/#option-1-using-aws-cross-account-roles","text":"To create a cross-account role for Hopsworks.ai, you need our AWS account id and the external id we created for you. You can find this information on the first screen of the cross-account configuration flow. Take note of the account id and external id and go to the Roles section of the IAM service in the AWS Management Console and select Create role . Creating the cross-account role instructions Select Another AWS account as trusted entity and fill in our AWS account id and the external id generated for you: Creating the cross-account role step 1 Go to the last step of the wizard, name the role and create it: Creating the cross-account role step 2 As a next step, you need to create an access policy to give Hopsworks.ai permissions to manage clusters in your organization's AWS account. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . Copy the permission JSON from the instructions: Adding the policy instructions Identify your newly created cross-account role in the Roles section of the IAM service in the AWS Management Console and select Add inline policy : Adding the inline policy step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 Copy the Role ARN from the summary of your cross-account role: Adding the inline policy step 4 Paste the Role ARN into Hopsworks.ai and click on Finish : Saving the cross-account role","title":"Option 1: Using AWS Cross-Account Roles"},{"location":"hopsworksai/aws/getting_started/#option-2-using-aws-access-keys","text":"You can either create a new IAM user or use an existing IAM user to create access keys for Hopsworks.ai. If you want to create a new IAM user, see Creating an IAM User in Your AWS Account . Warning We recommend using Cross-Account Roles instead of Access Keys whenever possible, see Option 1: Using AWS Cross-Account Roles . Hopsworks.ai requires a set of permissions to be able to launch clusters in your AWS account. The permissions can be granted by attaching an access policy to your IAM user. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . The required permissions are shown in the instructions. Copy them if you want to create a new access policy: Configuring access key instructions Add a new Inline policy to your AWS user: Configuring the access key on AWS step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 In the overview of your IAM user, select Create access key : Configuring the access key on AWS step 2 Copy the Access Key ID and the Secret Access Key : Configuring the access key on AWS step 3 Paste the Access Key ID and the Secret Access Key into Hopsworks.ai and click on Finish : Saving the access key pair","title":"Option 2: Using AWS Access Keys"},{"location":"hopsworksai/aws/getting_started/#step-2-creating-instance-profile","text":"Hopsworks cluster nodes need access to certain resources such as S3 bucket and CloudWatch. Follow the instructions in this guide to create an IAM instance profile with access to your S3 bucket: Guide When creating the policy, paste the following in the JSON tab. Replace BUCKET_NAME with the appropriate S3 bucket name. Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } ] }","title":"Step 2: Creating Instance profile"},{"location":"hopsworksai/aws/getting_started/#step-3-creating-storage","text":"The Hopsworks clusters deployed by hopsworks.ai store their data in an S3 bucket in your AWS account. To enable this you need to create an S3 bucket and an instance profile to give cluster nodes access to the bucket. Proceed to the S3 Management Console and click on Create bucket : Create an S3 bucket Name your bucket and select the region where your Hopsworks cluster will run. Click on Create bucket at the bottom of the page. Create an S3 bucket","title":"Step 3: Creating storage"},{"location":"hopsworksai/aws/getting_started/#step-4-create-an-ssh-key","text":"When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair .","title":"Step 4: Create an SSH key"},{"location":"hopsworksai/aws/getting_started/#step-41-create-a-new-key-pair","text":"Proceed to Key pairs in the EC2 console and click on Create key pair Create a key pair Name your key, select the file format you prefer and click on Create key pair . Create a key pair","title":"Step 4.1: Create a new key pair"},{"location":"hopsworksai/aws/getting_started/#step-42-import-a-key-pair","text":"Proceed to Key pairs in the EC2 console , click on Action and click on Import key pair Import a key pair Name your key pair, upload your public key and click on Import key pair . Import a key pair","title":"Step 4.2: Import a key pair"},{"location":"hopsworksai/aws/getting_started/#step-5-deploying-a-hopsworks-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Select the number of workers you want to start the cluster with (5). Select the Instance type (6) and Local storage size (7) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. Enter the name of the S3 bucket (8) you created above in S3 bucket . Note The S3 bucket you are using must be empty. Press Next (9): Create a Hopsworks cluster, general information Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the Instance Profile that you created above and click on Review and Submit : Choose the instance profile Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 5: Deploying a Hopsworks cluster"},{"location":"hopsworksai/aws/getting_started/#step-6-outside-access-to-the-feature-store","text":"By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store","title":"Step 6: Outside Access to the Feature Store"},{"location":"hopsworksai/aws/getting_started/#step-7-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Step 7: Next steps"},{"location":"hopsworksai/aws/instance_profile_permissions/","text":"Replace BUCKET_NAME with the appropriate S3 bucket name. Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } ] }","title":"Instance profile permissions"},{"location":"hopsworksai/aws/restrictive_permissions/","text":"Limiting AWS permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing Hopsworks.ai to only access resources in a specific VPC. Limiting the cross-account role permissions # Step 1: Create a VPC # To restrict Hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . The option VPC with a Single Public Subnet from the Launch VPC Wizard should work out of the box. Alternatively, an existing VPC such as the default VPC can be used and Hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC After you have created the VPC either Create a Security Group or use VPC's default. Note The Security Group and/or Network ACLs need to be configured so that at least port 80 is reachable from the internet otherwise you will have to use self signed certificate in your Hopsworks cluster. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Step 2: Create an instance profile # You need to create an instance profile that will identify all instances started by Hopsworks.ai. Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile. Step 3: Set permissions of the cross-account role # During the account setup for Hopsworks.ai, you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in Hopsworks.ai. Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:CreateImage\" , \"ec2:DescribeImages\" , \"ec2:DeregisterImage\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] } Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in Hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors. Step 5: Supporting multiple VPCs # The policy can be extended to give Hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values . Backup permissions # The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:CreateImage\" , \"ec2:DescribeImages\" , \"ec2:DeregisterImage\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , } Other removable permissions # The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } } Limiting the instance profile permissions # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }","title":"AWS"},{"location":"hopsworksai/aws/restrictive_permissions/#limiting-aws-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing Hopsworks.ai to only access resources in a specific VPC.","title":"Limiting AWS permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#step-1-create-a-vpc","text":"To restrict Hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . The option VPC with a Single Public Subnet from the Launch VPC Wizard should work out of the box. Alternatively, an existing VPC such as the default VPC can be used and Hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC After you have created the VPC either Create a Security Group or use VPC's default. Note The Security Group and/or Network ACLs need to be configured so that at least port 80 is reachable from the internet otherwise you will have to use self signed certificate in your Hopsworks cluster. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443.","title":"Step 1: Create a VPC"},{"location":"hopsworksai/aws/restrictive_permissions/#step-2-create-an-instance-profile","text":"You need to create an instance profile that will identify all instances started by Hopsworks.ai. Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile.","title":"Step 2: Create an instance profile"},{"location":"hopsworksai/aws/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for Hopsworks.ai, you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in Hopsworks.ai. Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:CreateImage\" , \"ec2:DescribeImages\" , \"ec2:DeregisterImage\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] }","title":"Step 3: Set permissions of the cross-account role"},{"location":"hopsworksai/aws/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in Hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors.","title":"Step 4: Create your Hopsworks instance"},{"location":"hopsworksai/aws/restrictive_permissions/#step-5-supporting-multiple-vpcs","text":"The policy can be extended to give Hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values .","title":"Step 5: Supporting multiple VPCs"},{"location":"hopsworksai/aws/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:CreateImage\" , \"ec2:DescribeImages\" , \"ec2:DeregisterImage\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , }","title":"Backup permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#other-removable-permissions","text":"The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }","title":"Other removable permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#limiting-the-instance-profile-permissions","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }","title":"Limiting the instance profile permissions"},{"location":"hopsworksai/aws/upgrade/","text":"Upgrade existing clusters on Hopsworks.ai (AWS) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available Step 1: Stop your cluster # You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available Step 2: Add upgrade permissions to your instance profile # We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] } Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Misconfigured upgrade permissions # During the upgrade process, Hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"AWS"},{"location":"hopsworksai/aws/upgrade/#upgrade-existing-clusters-on-hopsworksai-aws","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available","title":"Upgrade existing clusters on Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/upgrade/#step-1-stop-your-cluster","text":"You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available","title":"Step 1: Stop your cluster"},{"location":"hopsworksai/aws/upgrade/#step-2-add-upgrade-permissions-to-your-instance-profile","text":"We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Add upgrade permissions to your instance profile"},{"location":"hopsworksai/aws/upgrade/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 3: Run the upgrade process"},{"location":"hopsworksai/aws/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"hopsworksai/aws/upgrade/#error-1-misconfigured-upgrade-permissions","text":"During the upgrade process, Hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running","title":"Error 1: Misconfigured upgrade permissions"},{"location":"hopsworksai/aws/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"Error 2: Upgrade process error"},{"location":"hopsworksai/azure/aks_acr_integration/","text":"Integration with Azure AKS and ACR # This guide shows how to create a cluster in hopsworks.ai with integrated support for Azure Kubernetes Service (AKS) and Azure Container Registry (ACR). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. Hopsworks AKS and ACR integration have four requirements: A virtual network with access to AKS pods and the AKS API servers One Azure container registry configured in your account One AKS cluster Permissions to the ACR and AKS attached to a user-managed identity Note A public AKS cluster means the Kubernetes API server is accessible outside the virtual network it is deployed in. Similarly, a public ACR is accessible through the internet. User assigned managed identity (managed identity) # Note A user assigned managed identity (managed identity) can be created at the subscription level or to a specific resource group in a subscription. The managed identity is attached to the virtual machines that run inside your subscription (or resource group). Hence, the permissions only apply to services that run within your subscription (or resource group). The AKS and ACR integration requires some permissions to be attached to the managed identity used by the Hopsworks cluster. If you have already created a user assigned managed identity for the storage continue to Add role assignment to the managed identity using this identity. To set up the managed identity, go to the resource group where you will add the managed identity - this should be the same resource group you will deploy Hopsworks in. Click on the Add button. In the search dialog, enter \"user assigned managed identity\" . Click on Create . Then give a name to the managed identity and make sure that it is in the Region where you will deploy your cluster. Click on Review + create , and click on Create . Add role assignment to the managed identity # Go to the managed identity created above. Click on Azure role assignments in the left column. Click on Add role assignment . For the Scope select Resource group or Subscription depending on your preference. Select the Role AcrPull and click on Save . Repeat the same operation with the following roles: AcrPull AcrPush AcrDelete Azure Kubernetes Service User Role Warning You will also need to attach storage access permissions to the managed identity, see Creating and configuring a storage Once finished the role assignments should look similar to the picture below. AKS permissions Private AKS cluster and public ACR # This guide will step through setting up a private AKS cluster and a public ACR. Step 1: Create an AKS cluster # Go to Kubernetes services in the azure portal and click Add . Place the Kubernetes cluster in the same resource group as the Hopsworks cluster and choose a name for the Kubernetes cluster. AKS general configuration Next, click on the Authentication tab and verify the settings are identical to the picture below. Note Currently, AKS is only supported through managed identities. Contact the Logical Clocks sales team if you have a self-managed Kubernetes cluster. AKS authencation configuration Next, go to the networking tab and check Azure CNI . The portal will automatically fill in the IP address ranges for the Kubernetes virtual network. Take note of the virtual network name that is created, in this example the virtual network name was hopsworksstagevnet154. Lastly, check the Enable private cluster option. AKS network configuration Next, go to the Integrations tab. Under container registry click Create new. AKS create ACR Choose a name for the registry and select premium for the SKU. Then press OK . ACR configuration Next press Review + create , then click Create . To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, go to the registry you created. Go to the Retention (preview) tab and set Status from disabled to enabled . Set the retention policy for 7 days as in the figure below, then press save . ACR retention policy Step 2: create a virtual network for the Hopsworks cluster # Because the Kubernetes API service is private the Hopsworks cluster must be able to reach it over a private network. There are two options to integrate with a private AKS cluster. The first option ( A ) is to put the Hopsworks cluster in a pre-defined virtual network with a peering setup to the Kubernetes network. The second option ( B ) is to create a subnet inside the Kubernetes virtual network where the Hopsworks cluster will be placed. Option A : Peering setup # To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. Go to virtual networks and press create. Choose a name for the new virtual network and select the same resource group you are planning to use for your Hopsworks cluster. Next, go to the IP Addresses tab. Create an address space that does not overlap with the address space in the Kubernetes network. In the previous example, the automatically created Kubernetes network used the address space 10.0.0.0/8 . Hence, the address space 172.18.0.0/16 can safely be used. Next click Review + Create , then Create . Next, go to the created virtual network and go to the Peerings tab. Then click Add . Virtual network peering Choose a name for the peering link. Check the Traffic to remote virtual network as Allow , and Traffic forwarded from remote virtual network to Block . Virtual network peering configuration For the virtual network select the virtual network which was created by AKS, in our example this was hopsworksstagevnet154 . Then press Add . Virtual network peering configuration continuation The last step is to set up a DNS private link to be able to use DNS resolution for the Kubernetes API servers. Go to resource groups in the Azure portal and find the resource group of the Kubernetes cluster. This will be in the form of MC_ in this example it was MC_hopsworks-stage_hopsworks-aks_northeurope . Open the resource group and click on the DNS zone. Private DNS link setup In the left plane there is a tab called Virtual network links , click on the tab. Next press Add . Private DNS link configuration Choose a name for the private link and select the virtual network you will use for the Hopsworks cluster, then press OK. Private DNS link configuration The setup is now finalized and you can create the Hopsworks cluster. Option B : Subnet in AKS network # With this setup, the Hopsworks cluster will reside in the same virtual network as the AKS cluster. The difference is that a new subnet in the virtual network will be used for the Hopsworks cluster. To set up the subnet, first, go to the virtual network that was created by AKS. In our example, this was hopsworksstagevnet154. Next, go to the subnets tab. AKS subnet setup Press + Subnet . Choose a name for the subnet, for example, \"hopsworks\" and an IP range that does not overlap with the Kubernetes network. Then save. AKS subnet setup Create the Hopsworks cluster # This step assumes you are creating your Hopsworks cluster using hopsworks.ai. The AKS configuration can be set under the Managed containers tab. Set Use Azure AKS and Azure ACR as enabled. Two new fields will pop up. Fill them with the name of the container registry and the AKS you created above. In the previous example, we created an ACR with the name hopsworksaks and an AKS cluster with the name hopsaks-cluster . Hence, the configuration should look similar to the picture below Hopsworks AKS configuration In the virtual network tab, you have to select either the virtual network you created for the peering setup or the Kubernetes virtual network depending on which approach you choose. Under the subnet tab, you have to choose the default subnet if you choose the peering approach or the subnet you created if you choose to create a new subnet inside the AKS virtual network.","title":"Integration with AKS and ACR"},{"location":"hopsworksai/azure/aks_acr_integration/#integration-with-azure-aks-and-acr","text":"This guide shows how to create a cluster in hopsworks.ai with integrated support for Azure Kubernetes Service (AKS) and Azure Container Registry (ACR). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. Hopsworks AKS and ACR integration have four requirements: A virtual network with access to AKS pods and the AKS API servers One Azure container registry configured in your account One AKS cluster Permissions to the ACR and AKS attached to a user-managed identity Note A public AKS cluster means the Kubernetes API server is accessible outside the virtual network it is deployed in. Similarly, a public ACR is accessible through the internet.","title":"Integration with Azure AKS and ACR"},{"location":"hopsworksai/azure/aks_acr_integration/#user-assigned-managed-identity-managed-identity","text":"Note A user assigned managed identity (managed identity) can be created at the subscription level or to a specific resource group in a subscription. The managed identity is attached to the virtual machines that run inside your subscription (or resource group). Hence, the permissions only apply to services that run within your subscription (or resource group). The AKS and ACR integration requires some permissions to be attached to the managed identity used by the Hopsworks cluster. If you have already created a user assigned managed identity for the storage continue to Add role assignment to the managed identity using this identity. To set up the managed identity, go to the resource group where you will add the managed identity - this should be the same resource group you will deploy Hopsworks in. Click on the Add button. In the search dialog, enter \"user assigned managed identity\" . Click on Create . Then give a name to the managed identity and make sure that it is in the Region where you will deploy your cluster. Click on Review + create , and click on Create .","title":"User assigned managed identity (managed identity)"},{"location":"hopsworksai/azure/aks_acr_integration/#add-role-assignment-to-the-managed-identity","text":"Go to the managed identity created above. Click on Azure role assignments in the left column. Click on Add role assignment . For the Scope select Resource group or Subscription depending on your preference. Select the Role AcrPull and click on Save . Repeat the same operation with the following roles: AcrPull AcrPush AcrDelete Azure Kubernetes Service User Role Warning You will also need to attach storage access permissions to the managed identity, see Creating and configuring a storage Once finished the role assignments should look similar to the picture below. AKS permissions","title":"Add role assignment to the managed identity"},{"location":"hopsworksai/azure/aks_acr_integration/#private-aks-cluster-and-public-acr","text":"This guide will step through setting up a private AKS cluster and a public ACR.","title":"Private AKS cluster and public ACR"},{"location":"hopsworksai/azure/aks_acr_integration/#step-1-create-an-aks-cluster","text":"Go to Kubernetes services in the azure portal and click Add . Place the Kubernetes cluster in the same resource group as the Hopsworks cluster and choose a name for the Kubernetes cluster. AKS general configuration Next, click on the Authentication tab and verify the settings are identical to the picture below. Note Currently, AKS is only supported through managed identities. Contact the Logical Clocks sales team if you have a self-managed Kubernetes cluster. AKS authencation configuration Next, go to the networking tab and check Azure CNI . The portal will automatically fill in the IP address ranges for the Kubernetes virtual network. Take note of the virtual network name that is created, in this example the virtual network name was hopsworksstagevnet154. Lastly, check the Enable private cluster option. AKS network configuration Next, go to the Integrations tab. Under container registry click Create new. AKS create ACR Choose a name for the registry and select premium for the SKU. Then press OK . ACR configuration Next press Review + create , then click Create . To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, go to the registry you created. Go to the Retention (preview) tab and set Status from disabled to enabled . Set the retention policy for 7 days as in the figure below, then press save . ACR retention policy","title":"Step 1: Create an AKS cluster"},{"location":"hopsworksai/azure/aks_acr_integration/#step-2-create-a-virtual-network-for-the-hopsworks-cluster","text":"Because the Kubernetes API service is private the Hopsworks cluster must be able to reach it over a private network. There are two options to integrate with a private AKS cluster. The first option ( A ) is to put the Hopsworks cluster in a pre-defined virtual network with a peering setup to the Kubernetes network. The second option ( B ) is to create a subnet inside the Kubernetes virtual network where the Hopsworks cluster will be placed.","title":"Step 2: create a virtual network for the Hopsworks cluster"},{"location":"hopsworksai/azure/aks_acr_integration/#option-a-peering-setup","text":"To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. Go to virtual networks and press create. Choose a name for the new virtual network and select the same resource group you are planning to use for your Hopsworks cluster. Next, go to the IP Addresses tab. Create an address space that does not overlap with the address space in the Kubernetes network. In the previous example, the automatically created Kubernetes network used the address space 10.0.0.0/8 . Hence, the address space 172.18.0.0/16 can safely be used. Next click Review + Create , then Create . Next, go to the created virtual network and go to the Peerings tab. Then click Add . Virtual network peering Choose a name for the peering link. Check the Traffic to remote virtual network as Allow , and Traffic forwarded from remote virtual network to Block . Virtual network peering configuration For the virtual network select the virtual network which was created by AKS, in our example this was hopsworksstagevnet154 . Then press Add . Virtual network peering configuration continuation The last step is to set up a DNS private link to be able to use DNS resolution for the Kubernetes API servers. Go to resource groups in the Azure portal and find the resource group of the Kubernetes cluster. This will be in the form of MC_ in this example it was MC_hopsworks-stage_hopsworks-aks_northeurope . Open the resource group and click on the DNS zone. Private DNS link setup In the left plane there is a tab called Virtual network links , click on the tab. Next press Add . Private DNS link configuration Choose a name for the private link and select the virtual network you will use for the Hopsworks cluster, then press OK. Private DNS link configuration The setup is now finalized and you can create the Hopsworks cluster.","title":"Option A: Peering setup"},{"location":"hopsworksai/azure/aks_acr_integration/#option-b-subnet-in-aks-network","text":"With this setup, the Hopsworks cluster will reside in the same virtual network as the AKS cluster. The difference is that a new subnet in the virtual network will be used for the Hopsworks cluster. To set up the subnet, first, go to the virtual network that was created by AKS. In our example, this was hopsworksstagevnet154. Next, go to the subnets tab. AKS subnet setup Press + Subnet . Choose a name for the subnet, for example, \"hopsworks\" and an IP range that does not overlap with the Kubernetes network. Then save. AKS subnet setup","title":"Option B: Subnet in AKS network"},{"location":"hopsworksai/azure/aks_acr_integration/#create-the-hopsworks-cluster","text":"This step assumes you are creating your Hopsworks cluster using hopsworks.ai. The AKS configuration can be set under the Managed containers tab. Set Use Azure AKS and Azure ACR as enabled. Two new fields will pop up. Fill them with the name of the container registry and the AKS you created above. In the previous example, we created an ACR with the name hopsworksaks and an AKS cluster with the name hopsaks-cluster . Hence, the configuration should look similar to the picture below Hopsworks AKS configuration In the virtual network tab, you have to select either the virtual network you created for the peering setup or the Kubernetes virtual network depending on which approach you choose. Under the subnet tab, you have to choose the default subnet if you choose the peering approach or the subnet you created if you choose to create a new subnet inside the AKS virtual network.","title":"Create the Hopsworks cluster"},{"location":"hopsworksai/azure/cluster_creation/","text":"Getting started with Hopsworks.ai (Azure) # This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai Step 1 starting to create a cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Select the number of workers you want to start the cluster with (6). Select the Instance type (7) and Local storage size (8) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by Hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (9) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (10). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next (11): General configuration Step 3 select a SSH key # When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key Step 4 select the User assigned managed identity: # In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity Step 5 set the backup retention policy: # Note This step is only accessible to enterprise users. To back up the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 6 Virtual network selection # In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network Step 7 Subnet selection # If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step Hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet Step 8 Network Security group selection # In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let Hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Choose security group Step 9 User management selection # In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 10 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 11 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Azure"},{"location":"hopsworksai/azure/cluster_creation/#getting-started-with-hopsworksai-azure","text":"This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"hopsworksai/azure/cluster_creation/#step-2-setting-the-general-information","text":"Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Select the number of workers you want to start the cluster with (6). Select the Instance type (7) and Local storage size (8) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by Hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (9) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (10). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next (11): General configuration","title":"Step 2 setting the General information"},{"location":"hopsworksai/azure/cluster_creation/#step-3-select-a-ssh-key","text":"When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key","title":"Step 3 select a SSH key"},{"location":"hopsworksai/azure/cluster_creation/#step-4-select-the-user-assigned-managed-identity","text":"In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity","title":"Step 4 select the User assigned managed identity:"},{"location":"hopsworksai/azure/cluster_creation/#step-5-set-the-backup-retention-policy","text":"Note This step is only accessible to enterprise users. To back up the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 5 set the backup retention policy:"},{"location":"hopsworksai/azure/cluster_creation/#step-6-virtual-network-selection","text":"In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network","title":"Step 6 Virtual network selection"},{"location":"hopsworksai/azure/cluster_creation/#step-7-subnet-selection","text":"If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step Hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet","title":"Step 7 Subnet selection"},{"location":"hopsworksai/azure/cluster_creation/#step-8-network-security-group-selection","text":"In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let Hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Choose security group","title":"Step 8 Network Security group selection"},{"location":"hopsworksai/azure/cluster_creation/#step-9-user-management-selection","text":"In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 9 User management selection"},{"location":"hopsworksai/azure/cluster_creation/#step-10-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 10 add tags to your instances."},{"location":"hopsworksai/azure/cluster_creation/#step-11-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 11 Review and create"},{"location":"hopsworksai/azure/getting_started/","text":"Getting started with Hopsworks.ai (Azure) # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Azure account. Step 1: Connecting your Azure account # Hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for Hopsworks.ai granting access to either a subscription or resource group. Step 1.0: Prerequisite # For Hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. You can verify that they are registered by going to your subscription in the Azure portal and click on Resource providers . If one of the resource providers is not registered select it and click on Register . Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y Step 1.1: Creating a service principal for Hopsworks.ai # On Hopsworks.ai, go to Settings/Cloud Accounts and choose to Configure Azure: Cloud account settings Select Add subscription key : Add subscription keys The Azure account configuration will show you the required steps and permissions. Ensure that you have the Azure CLI installed Install the Azure CLI and are logged in Sign in with Azure CLI . Copy the Azure CLI command from the first step and open a terminal: Connect your Azure Account Paste the command into the terminal and execute it: Add service principal At this point, you might get the following error message. This means that your Azure user does not have sufficient permissions to add the service principal. In this case, please ask your Azure administrator to add it for you or give you the required permissions. Error az ad sp create --id d4abcc44-2c40-40bd-9bba-986df591c28f When using this permission, the backing application of the service principal being created must in the local tenant. Step 1.2: Creating a custom role for Hopsworks.ai # Proceed to the Azure Portal and open either a Subscription or Resource Group that you want to use for Hopsworks.ai. Select Add and choose Add custom role . Note Granting access to a Subscription will grant access to all Resource Groups in that Subscription . If you are uncertain if that is what you want, then start with a Resource Group . Add custom role Name the role and proceed to Assignable scopes : Name custom role Ensure the scope is set to the Subscription or Resource Group you want to use. You can change it here if required. Proceed to the JSON tab: Review assignable scope Select Edit and replace the actions part of the JSON with the one from Hopsworks.ai Azure account configuration workflow: Hopsworks.ai permission list Note If the access rights provided by Hopsworks.ai Azure account configuration workflow are too permissive, you can go to Limiting Azure permissions for more details on how to limit the permissions. Press Save , proceed to Review + create and create the role: Update permission JSON Step 1.3: Assigning the custom role to Hopsworks.ai # Back in the Subscription or Resource Group overview, select Add and choose Add role assignment : Add role assignment Choose the custom role you just created, select User, group, or service principal to Assign access to and select the hopsworks.ai service principal. Press Save : Configure Hopsworks.ai as role assignment Go back to the Hopsworks.ai Azure account configuration workflow and proceed to the next step. Copy the CLI command shown: Configure subscription and tenant id Paste the CLI command into your terminal and execute it. Note that you might have multiple entries listed here. If so, ensure that you pick the subscription that you want to use. Show subscription and tenant id Copy the value of id and paste it into the Subscription id field on Hopsworks.ai. Go back to the terminal and copy the value of tenantId . Ensure to NOT use the tenantId under managedByTenants . Paste the value into the Tenant ID field on Hopsworks.ai and press Finish . Congratulations, you have successfully connected you Azure account to Hopsworks.ai. Store subscription and tenant id Step 2: Creating and configuring a storage # The Hopsworks clusters deployed by hopsworks.ai store their data in a container in your Azure account. To enable this you need to perform the following operations Create a restrictive role to limit access to the storage account Create a User Assigned Managed Identity Create a storage account and give Hopsworks clusters access to the storage using the restrictive role Step 2.1: Creating a Restrictive Role for Accessing Storage # Similarly to Step 1.2 create a new role named Hopsworks Storage Role . Add the following permissions to the role \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ] Note Some of these permissions can be removed at the cost of Hopsworks features, see Limiting Azure permissions for more details. Step 2.2: Creating a User Assigned Managed Identity # Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add . Add to resource group Search for User Assigned Managed Identity and click on it. Search User Assigned Managed Identity Click on Create . Then, select the Location you want to use and name the identity. Click on Review + create . Finally click on Create . Create a User Assigned Managed Identity Step 2.3: Creating a Storage account # Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add . Add to resource group Search for Storage account and click on it. Search Storage Account Identity Click on Create , name your storage account, select the Location you want to use and click on Review + create . Finally click on Create . Create a Storage Account Step 2.4: Give the Managed Identity access to the storage # Proceed to the Storage Account you just created and click on Access Control (IAM) (1). Click on Add (2), then click on Add role assignment (3). In Role select Hopsworks Storage Role (4). In Assign access to select User assigned managed identity (5). Select the identity you created in step 2.1 (6). Click on Save (7). Add role assignment to storage Step 3: Adding a ssh key to your resource group # When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. For this purpose you need to add a ssh key to your resource group. Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add . Add to resource group Search for SSH Key and click on it. Click on Create. Then, name your key pair and choose between Generate a new key pair and Upload existing public key . Click on Review + create . Finally click on Create . Add to resource group Step 4: Deploying a Hopsworks cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Select the number of workers you want to start the cluster with (6). Select the Instance type (7) and Local storage size (8) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. Select the storage account (9) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (10), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next (11): General configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above and click on Review and Create : Choose the User assigned managed identity Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster Step 5: Outside Access to the Feature Store # By default, only the Hopsworks REST API (and UI) is accessible by clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to Services tab, selecting a service and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store Step 6: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Azure"},{"location":"hopsworksai/azure/getting_started/#getting-started-with-hopsworksai-azure","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Azure account.","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/getting_started/#step-1-connecting-your-azure-account","text":"Hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for Hopsworks.ai granting access to either a subscription or resource group.","title":"Step 1: Connecting your Azure account"},{"location":"hopsworksai/azure/getting_started/#step-10-prerequisite","text":"For Hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. You can verify that they are registered by going to your subscription in the Azure portal and click on Resource providers . If one of the resource providers is not registered select it and click on Register . Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y","title":"Step 1.0: Prerequisite"},{"location":"hopsworksai/azure/getting_started/#step-11-creating-a-service-principal-for-hopsworksai","text":"On Hopsworks.ai, go to Settings/Cloud Accounts and choose to Configure Azure: Cloud account settings Select Add subscription key : Add subscription keys The Azure account configuration will show you the required steps and permissions. Ensure that you have the Azure CLI installed Install the Azure CLI and are logged in Sign in with Azure CLI . Copy the Azure CLI command from the first step and open a terminal: Connect your Azure Account Paste the command into the terminal and execute it: Add service principal At this point, you might get the following error message. This means that your Azure user does not have sufficient permissions to add the service principal. In this case, please ask your Azure administrator to add it for you or give you the required permissions. Error az ad sp create --id d4abcc44-2c40-40bd-9bba-986df591c28f When using this permission, the backing application of the service principal being created must in the local tenant.","title":"Step 1.1: Creating a service principal for Hopsworks.ai"},{"location":"hopsworksai/azure/getting_started/#step-12-creating-a-custom-role-for-hopsworksai","text":"Proceed to the Azure Portal and open either a Subscription or Resource Group that you want to use for Hopsworks.ai. Select Add and choose Add custom role . Note Granting access to a Subscription will grant access to all Resource Groups in that Subscription . If you are uncertain if that is what you want, then start with a Resource Group . Add custom role Name the role and proceed to Assignable scopes : Name custom role Ensure the scope is set to the Subscription or Resource Group you want to use. You can change it here if required. Proceed to the JSON tab: Review assignable scope Select Edit and replace the actions part of the JSON with the one from Hopsworks.ai Azure account configuration workflow: Hopsworks.ai permission list Note If the access rights provided by Hopsworks.ai Azure account configuration workflow are too permissive, you can go to Limiting Azure permissions for more details on how to limit the permissions. Press Save , proceed to Review + create and create the role: Update permission JSON","title":"Step 1.2: Creating a custom role for Hopsworks.ai"},{"location":"hopsworksai/azure/getting_started/#step-13-assigning-the-custom-role-to-hopsworksai","text":"Back in the Subscription or Resource Group overview, select Add and choose Add role assignment : Add role assignment Choose the custom role you just created, select User, group, or service principal to Assign access to and select the hopsworks.ai service principal. Press Save : Configure Hopsworks.ai as role assignment Go back to the Hopsworks.ai Azure account configuration workflow and proceed to the next step. Copy the CLI command shown: Configure subscription and tenant id Paste the CLI command into your terminal and execute it. Note that you might have multiple entries listed here. If so, ensure that you pick the subscription that you want to use. Show subscription and tenant id Copy the value of id and paste it into the Subscription id field on Hopsworks.ai. Go back to the terminal and copy the value of tenantId . Ensure to NOT use the tenantId under managedByTenants . Paste the value into the Tenant ID field on Hopsworks.ai and press Finish . Congratulations, you have successfully connected you Azure account to Hopsworks.ai. Store subscription and tenant id","title":"Step 1.3: Assigning the custom role to Hopsworks.ai"},{"location":"hopsworksai/azure/getting_started/#step-2-creating-and-configuring-a-storage","text":"The Hopsworks clusters deployed by hopsworks.ai store their data in a container in your Azure account. To enable this you need to perform the following operations Create a restrictive role to limit access to the storage account Create a User Assigned Managed Identity Create a storage account and give Hopsworks clusters access to the storage using the restrictive role","title":"Step 2: Creating and configuring a storage"},{"location":"hopsworksai/azure/getting_started/#step-21-creating-a-restrictive-role-for-accessing-storage","text":"Similarly to Step 1.2 create a new role named Hopsworks Storage Role . Add the following permissions to the role \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ] Note Some of these permissions can be removed at the cost of Hopsworks features, see Limiting Azure permissions for more details.","title":"Step 2.1: Creating a Restrictive Role for Accessing Storage"},{"location":"hopsworksai/azure/getting_started/#step-22-creating-a-user-assigned-managed-identity","text":"Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add . Add to resource group Search for User Assigned Managed Identity and click on it. Search User Assigned Managed Identity Click on Create . Then, select the Location you want to use and name the identity. Click on Review + create . Finally click on Create . Create a User Assigned Managed Identity","title":"Step 2.2: Creating a User Assigned Managed Identity"},{"location":"hopsworksai/azure/getting_started/#step-23-creating-a-storage-account","text":"Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add . Add to resource group Search for Storage account and click on it. Search Storage Account Identity Click on Create , name your storage account, select the Location you want to use and click on Review + create . Finally click on Create . Create a Storage Account","title":"Step 2.3: Creating a Storage account"},{"location":"hopsworksai/azure/getting_started/#step-24-give-the-managed-identity-access-to-the-storage","text":"Proceed to the Storage Account you just created and click on Access Control (IAM) (1). Click on Add (2), then click on Add role assignment (3). In Role select Hopsworks Storage Role (4). In Assign access to select User assigned managed identity (5). Select the identity you created in step 2.1 (6). Click on Save (7). Add role assignment to storage","title":"Step 2.4: Give the Managed Identity access to the storage"},{"location":"hopsworksai/azure/getting_started/#step-3-adding-a-ssh-key-to-your-resource-group","text":"When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. For this purpose you need to add a ssh key to your resource group. Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add . Add to resource group Search for SSH Key and click on it. Click on Create. Then, name your key pair and choose between Generate a new key pair and Upload existing public key . Click on Review + create . Finally click on Create . Add to resource group","title":"Step 3: Adding a ssh key to your resource group"},{"location":"hopsworksai/azure/getting_started/#step-4-deploying-a-hopsworks-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Select the number of workers you want to start the cluster with (6). Select the Instance type (7) and Local storage size (8) for the worker nodes . Note It is possible to add or remove workers once the cluster is running. Select the storage account (9) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (10), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next (11): General configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above and click on Review and Create : Choose the User assigned managed identity Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster","title":"Step 4: Deploying a Hopsworks cluster"},{"location":"hopsworksai/azure/getting_started/#step-5-outside-access-to-the-feature-store","text":"By default, only the Hopsworks REST API (and UI) is accessible by clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to Services tab, selecting a service and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store","title":"Step 5: Outside Access to the Feature Store"},{"location":"hopsworksai/azure/getting_started/#step-6-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Step 6: Next steps"},{"location":"hopsworksai/azure/restrictive_permissions/","text":"Limiting Azure permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege. Limiting the cross-account role permissions # Step 1: Create a virtual network and subnet # To restrict Hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps. Step 2: Create a network security group # To restrict Hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For Hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Step 3: Set permissions of the cross-account role # During the account setup for Hopsworks.ai, you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ] Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in Hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration. Backup permissions # The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ] Other removable permissions # The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in Azure. \"actions\" : [ \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , ] The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ] Limiting the User Assigned Managed Identity permissions # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this replace the permissions given in the getting started instructions by the following: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ]","title":"Azure"},{"location":"hopsworksai/azure/restrictive_permissions/#limiting-azure-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege.","title":"Limiting Azure permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#step-1-create-a-virtual-network-and-subnet","text":"To restrict Hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps.","title":"Step 1: Create a virtual network and subnet"},{"location":"hopsworksai/azure/restrictive_permissions/#step-2-create-a-network-security-group","text":"To restrict Hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For Hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443.","title":"Step 2: Create a network security group"},{"location":"hopsworksai/azure/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for Hopsworks.ai, you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ]","title":"Step 3: Set permissions of the cross-account role"},{"location":"hopsworksai/azure/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in Hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration.","title":"Step 4: Create your Hopsworks instance"},{"location":"hopsworksai/azure/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ]","title":"Backup permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#other-removable-permissions","text":"The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in Azure. \"actions\" : [ \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , ] The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ]","title":"Other removable permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#limiting-the-user-assigned-managed-identity-permissions","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this replace the permissions given in the getting started instructions by the following: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ]","title":"Limiting the User Assigned Managed Identity permissions"},{"location":"hopsworksai/azure/upgrade/","text":"Upgrade existing clusters on Hopsworks.ai (Azure) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available Step 1: Stop your cluster # You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available Step 2: Add upgrade permissions to your user assigned managed identity # We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster Step 2.1: Add custom role for upgrade permissions # Once you get the names of the resource group and user-assigned managed identity, follow the same steps as in getting started to add a custom role . First, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade Step 2.2: Assign the custom role to your user-assigned managed identity # Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect. Step 3: Add disk read permissions to your role connected to Hopsworks.ai # We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Step 4: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing permissions error # If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"Azure"},{"location":"hopsworksai/azure/upgrade/#upgrade-existing-clusters-on-hopsworksai-azure","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available","title":"Upgrade existing clusters on Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/upgrade/#step-1-stop-your-cluster","text":"You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available","title":"Step 1: Stop your cluster"},{"location":"hopsworksai/azure/upgrade/#step-2-add-upgrade-permissions-to-your-user-assigned-managed-identity","text":"We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster","title":"Step 2: Add upgrade permissions to your user assigned managed identity"},{"location":"hopsworksai/azure/upgrade/#step-21-add-custom-role-for-upgrade-permissions","text":"Once you get the names of the resource group and user-assigned managed identity, follow the same steps as in getting started to add a custom role . First, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade","title":"Step 2.1: Add custom role for upgrade permissions"},{"location":"hopsworksai/azure/upgrade/#step-22-assign-the-custom-role-to-your-user-assigned-managed-identity","text":"Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect.","title":"Step 2.2: Assign the custom role to your user-assigned managed identity"},{"location":"hopsworksai/azure/upgrade/#step-3-add-disk-read-permissions-to-your-role-connected-to-hopsworksai","text":"We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai","title":"Step 3: Add disk read permissions to your role connected to Hopsworks.ai"},{"location":"hopsworksai/azure/upgrade/#step-4-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 4: Run the upgrade process"},{"location":"hopsworksai/azure/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"hopsworksai/azure/upgrade/#error-1-missing-permissions-error","text":"If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process","title":"Error 1: Missing permissions error"},{"location":"hopsworksai/azure/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"Error 2: Upgrade process error"},{"location":"integrations/hdinsight/","text":"Configure HDInsight for the Hopsworks Feature Store # To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information. Step 1: Set up a Hopsworks API key # In order for HDInsight clusters to be able to communicate with the Hopsworks Feature Store, the clients running on HDInsight need a Hopsworks API key. In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Make sure you have the key handy for the next steps. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Step 2: Use a script action to install the Feature Store connector # HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks Step 3: Configure HDInsight for Feature Store access # The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store. Step 5: Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Azure HDInsight"},{"location":"integrations/hdinsight/#configure-hdinsight-for-the-hopsworks-feature-store","text":"To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information.","title":"Configure HDInsight for the Hopsworks Feature Store"},{"location":"integrations/hdinsight/#step-1-set-up-a-hopsworks-api-key","text":"In order for HDInsight clusters to be able to communicate with the Hopsworks Feature Store, the clients running on HDInsight need a Hopsworks API key. In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Make sure you have the key handy for the next steps. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Step 1: Set up a Hopsworks API key"},{"location":"integrations/hdinsight/#step-2-use-a-script-action-to-install-the-feature-store-connector","text":"HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks","title":"Step 2:  Use a script action to install the Feature Store connector"},{"location":"integrations/hdinsight/#step-3-configure-hdinsight-for-feature-store-access","text":"The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store.","title":"Step 3: Configure HDInsight for Feature Store access"},{"location":"integrations/hdinsight/#step-5-connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Step 5: Connect to the Feature Store"},{"location":"integrations/hdinsight/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/mlstudio_designer/","text":"Azure Machine Learning Designer Integration # Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connect to the Feature Store # To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[hive]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Designer"},{"location":"integrations/mlstudio_designer/#azure-machine-learning-designer-integration","text":"Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Designer Integration"},{"location":"integrations/mlstudio_designer/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/mlstudio_designer/#connect-to-the-feature-store","text":"To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[hive]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline","title":"Connect to the Feature Store"},{"location":"integrations/mlstudio_designer/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/mlstudio_notebooks/","text":"Azure Machine Learning Notebooks Integration # Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connect from an Azure Machine Learning Notebook # To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Notebooks"},{"location":"integrations/mlstudio_notebooks/#azure-machine-learning-notebooks-integration","text":"Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Notebooks Integration"},{"location":"integrations/mlstudio_notebooks/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/mlstudio_notebooks/#connect-from-an-azure-machine-learning-notebook","text":"To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook","title":"Connect from an Azure Machine Learning Notebook"},{"location":"integrations/mlstudio_notebooks/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"integrations/mlstudio_notebooks/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Connect to the Feature Store"},{"location":"integrations/mlstudio_notebooks/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/python/","text":"Python Environments (Local or Kubeflow) # Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard. Create a file called featurestore.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='hive' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services . Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Python"},{"location":"integrations/python/#python-environments-local-or-kubeflow","text":"Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow.","title":"Python Environments (Local or Kubeflow)"},{"location":"integrations/python/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard. Create a file called featurestore.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/python/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"integrations/python/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='hive' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services .","title":"Connect to the Feature Store"},{"location":"integrations/python/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/sagemaker/","text":"AWS SageMaker Integration # Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key on AWS # The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks. Identify your SageMaker role # You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance Store the API key # You have two options to make your API key accessible from SageMaker: Option 1: Using the AWS Systems Manager Parameter Store # Store the API key in the AWS Systems Manager Parameter Store # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store Grant access to the Parameter Store from the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role Option 2: Using the AWS Secrets Manager # Store the API key in the AWS Secrets Manager # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager Grant access to the SecretsManager to the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances. Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"hive\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS . Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"AWS Sagemaker"},{"location":"integrations/sagemaker/#aws-sagemaker-integration","text":"Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker.","title":"AWS SageMaker Integration"},{"location":"integrations/sagemaker/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/sagemaker/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"integrations/sagemaker/#store-the-api-key-on-aws","text":"The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks.","title":"Store the API key on AWS"},{"location":"integrations/sagemaker/#identify-your-sagemaker-role","text":"You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance","title":"Identify your SageMaker role"},{"location":"integrations/sagemaker/#store-the-api-key","text":"You have two options to make your API key accessible from SageMaker:","title":"Store the API key"},{"location":"integrations/sagemaker/#option-1-using-the-aws-systems-manager-parameter-store","text":"","title":"Option 1: Using the AWS Systems Manager Parameter Store"},{"location":"integrations/sagemaker/#store-the-api-key-in-the-aws-systems-manager-parameter-store","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store","title":"Store the API key in the AWS Systems Manager Parameter Store"},{"location":"integrations/sagemaker/#grant-access-to-the-parameter-store-from-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role","title":"Grant access to the Parameter Store from the SageMaker notebook role"},{"location":"integrations/sagemaker/#option-2-using-the-aws-secrets-manager","text":"","title":"Option 2: Using the AWS Secrets Manager"},{"location":"integrations/sagemaker/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager","title":"Store the API key in the AWS Secrets Manager"},{"location":"integrations/sagemaker/#grant-access-to-the-secretsmanager-to-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role","title":"Grant access to the SecretsManager to the SageMaker notebook role"},{"location":"integrations/sagemaker/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances.","title":"Install HSFS"},{"location":"integrations/sagemaker/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"hive\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS .","title":"Connect to the Feature Store"},{"location":"integrations/sagemaker/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/spark/","text":"Spark Integration # Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster. Download the Hopsworks Client Jars # In the Feature Store UI, select the integration tab and then select the Spark tab. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the client libraries for HopsHive and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster Download the certificates # Download the certificates from the same Spark tab in the Feature Store UI. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well. Configure your Spark cluster # Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hop.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.truststore.name trustStore.jks spark.sql.hive.metastore.jars [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars should point to the path with the Hive Jars which can be found in the clients.tar.gz . PySpark # To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Generating an API Key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select Api keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The created API-Key should at least have the following scopes: featurestore project job API-Keys can be generated in the User Settings on Hopsworks Info You are only ably to retrieve the API Key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connecting to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above. Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Spark"},{"location":"integrations/spark/#spark-integration","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Spark Integration"},{"location":"integrations/spark/#download-the-hopsworks-client-jars","text":"In the Feature Store UI, select the integration tab and then select the Spark tab. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the client libraries for HopsHive and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster","title":"Download the Hopsworks Client Jars"},{"location":"integrations/spark/#download-the-certificates","text":"Download the certificates from the same Spark tab in the Feature Store UI. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well.","title":"Download the certificates"},{"location":"integrations/spark/#configure-your-spark-cluster","text":"Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hop.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.truststore.name trustStore.jks spark.sql.hive.metastore.jars [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars should point to the path with the Hive Jars which can be found in the clients.tar.gz .","title":"Configure your Spark cluster"},{"location":"integrations/spark/#pyspark","text":"To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"PySpark"},{"location":"integrations/spark/#generating-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select Api keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The created API-Key should at least have the following scopes: featurestore project job API-Keys can be generated in the User Settings on Hopsworks Info You are only ably to retrieve the API Key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generating an API Key"},{"location":"integrations/spark/#connecting-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above.","title":"Connecting to the Feature Store"},{"location":"integrations/spark/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/databricks/api_key/","text":"Hopsworks API key # In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key # AWS # Option 1: Using the AWS Systems Manager Parameter Store # Store the API key in the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks cluster that should access the Feature Store. Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store Option 2: Using the AWS Secrets Manager # Store the API key in the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager Azure # On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store. Next Steps # Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Hopsworks API Key"},{"location":"integrations/databricks/api_key/#hopsworks-api-key","text":"In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key.","title":"Hopsworks API key"},{"location":"integrations/databricks/api_key/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/databricks/api_key/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"integrations/databricks/api_key/#store-the-api-key","text":"","title":"Store the API key"},{"location":"integrations/databricks/api_key/#aws","text":"","title":"AWS"},{"location":"integrations/databricks/api_key/#option-1-using-the-aws-systems-manager-parameter-store","text":"Store the API key in the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks cluster that should access the Feature Store. Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store","title":"Option 1: Using the AWS Systems Manager Parameter Store"},{"location":"integrations/databricks/api_key/#option-2-using-the-aws-secrets-manager","text":"Store the API key in the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager","title":"Option 2: Using the AWS Secrets Manager"},{"location":"integrations/databricks/api_key/#azure","text":"On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store.","title":"Azure"},{"location":"integrations/databricks/api_key/#next-steps","text":"Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/databricks/configuration/","text":"Databricks Integration # Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step. Prerequisites # In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks. Networking # If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the Hopsworks.ai VPC/VNet. Hopsworks API key # In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks . Databricks API key # Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure. Register a new Databricks Instance # Users can register a new Databricks instance by navigating to the Integrations tab of a project Feature Store. Registering a Databricks instance requires adding the instance address and the API key. The instance address should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of. Databricks Cluster # A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 6 is suggested to be able to use the full suite of Hopsworks Feature Store capabilities. Configure a cluster # Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above. Connecting to the Feature Store # At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = \"featurestore.key\" , # For Azure, store the API key locally hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Configuration"},{"location":"integrations/databricks/configuration/#databricks-integration","text":"Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step.","title":"Databricks Integration"},{"location":"integrations/databricks/configuration/#prerequisites","text":"In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks.","title":"Prerequisites"},{"location":"integrations/databricks/configuration/#networking","text":"If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the Hopsworks.ai VPC/VNet.","title":"Networking"},{"location":"integrations/databricks/configuration/#hopsworks-api-key","text":"In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks .","title":"Hopsworks API key"},{"location":"integrations/databricks/configuration/#databricks-api-key","text":"Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure.","title":"Databricks API key"},{"location":"integrations/databricks/configuration/#register-a-new-databricks-instance","text":"Users can register a new Databricks instance by navigating to the Integrations tab of a project Feature Store. Registering a Databricks instance requires adding the instance address and the API key. The instance address should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of.","title":"Register a new Databricks Instance"},{"location":"integrations/databricks/configuration/#databricks-cluster","text":"A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 6 is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.","title":"Databricks Cluster"},{"location":"integrations/databricks/configuration/#configure-a-cluster","text":"Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above.","title":"Configure a cluster"},{"location":"integrations/databricks/configuration/#connecting-to-the-feature-store","text":"At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = \"featurestore.key\" , # For Azure, store the API key locally hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Connecting to the Feature Store"},{"location":"integrations/databricks/configuration/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/databricks/networking/","text":"Networking # In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster. AWS # Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are working on Hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details Azure # Step 1: Set up VNet peering between Hopsworks and Databricks # VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on Hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings Step 2: Configure the Network Security Group # The Network Security Group of the Feature Store on Azure needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Ensure that ports 443 , 9083 , 9085 , 8020 and 50010 are reachable from the Databricks cluster Network Security Group . Hopsworks.ai If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services . Next Steps # Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/databricks/networking/#networking","text":"In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster.","title":"Networking"},{"location":"integrations/databricks/networking/#aws","text":"","title":"AWS"},{"location":"integrations/databricks/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are working on Hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"integrations/databricks/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details","title":"Step 2: Configure the Security Group"},{"location":"integrations/databricks/networking/#azure","text":"","title":"Azure"},{"location":"integrations/databricks/networking/#step-1-set-up-vnet-peering-between-hopsworks-and-databricks","text":"VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on Hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings","title":"Step 1: Set up VNet peering between Hopsworks and Databricks"},{"location":"integrations/databricks/networking/#step-2-configure-the-network-security-group","text":"The Network Security Group of the Feature Store on Azure needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Ensure that ports 443 , 9083 , 9085 , 8020 and 50010 are reachable from the Databricks cluster Network Security Group . Hopsworks.ai If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services .","title":"Step 2: Configure the Network Security Group"},{"location":"integrations/databricks/networking/#next-steps","text":"Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/emr/emr_configuration/","text":"Configure EMR for the Hopsworks Feature Store # To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide. Step 1: Set up a Hopsworks API key # In order for EMR clusters to be able to communicate with the Hopsworks Feature Store, the clients running on EMR need to be able to access a Hopsworks API key. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: project API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Store the API key in the AWS Secrets Manager # In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret Grant access to the secret to the EMR EC2 instance profile # Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager Step 2: Configure your EMR cluster # Add the Hopsworks Feature Store configuration to your EMR cluster # In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"core-site\" , \"Properties\" : { \"fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"hops.ipc.server.ssl.enabled\" : true , \"hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" } }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.sql.hive.metastore.jars\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" } }, { \"Classification\" : \"spark-hive-site\" , \"Properties\" : { \"hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } } ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store Add the Bootstrap Action to your EMR cluster # EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store. Next Steps # If you use Python, then install the HSFS library . The Scala version of the library has already been installed to your EMR cluster. Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"integrations/emr/emr_configuration/#configure-emr-for-the-hopsworks-feature-store","text":"To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide.","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"integrations/emr/emr_configuration/#step-1-set-up-a-hopsworks-api-key","text":"In order for EMR clusters to be able to communicate with the Hopsworks Feature Store, the clients running on EMR need to be able to access a Hopsworks API key.","title":"Step 1: Set up a Hopsworks API key"},{"location":"integrations/emr/emr_configuration/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: project API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/emr/emr_configuration/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret","title":"Store the API key in the AWS Secrets Manager"},{"location":"integrations/emr/emr_configuration/#grant-access-to-the-secret-to-the-emr-ec2-instance-profile","text":"Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager","title":"Grant access to the secret to the EMR EC2 instance profile"},{"location":"integrations/emr/emr_configuration/#step-2-configure-your-emr-cluster","text":"","title":"Step 2: Configure your EMR cluster"},{"location":"integrations/emr/emr_configuration/#add-the-hopsworks-feature-store-configuration-to-your-emr-cluster","text":"In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"core-site\" , \"Properties\" : { \"fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"hops.ipc.server.ssl.enabled\" : true , \"hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" } }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.sql.hive.metastore.jars\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" } }, { \"Classification\" : \"spark-hive-site\" , \"Properties\" : { \"hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } } ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store","title":"Add the Hopsworks Feature Store configuration to your EMR cluster"},{"location":"integrations/emr/emr_configuration/#add-the-bootstrap-action-to-your-emr-cluster","text":"EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store.","title":"Add the Bootstrap Action to your EMR cluster"},{"location":"integrations/emr/emr_configuration/#next-steps","text":"If you use Python, then install the HSFS library . The Scala version of the library has already been installed to your EMR cluster. Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Next Steps"},{"location":"integrations/emr/networking/","text":"Networking # In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store. Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are on Hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups Next Steps # Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/emr/networking/#networking","text":"In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/emr/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are on Hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"integrations/emr/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups","title":"Step 2: Configure the Security Group"},{"location":"integrations/emr/networking/#next-steps","text":"Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"}]}